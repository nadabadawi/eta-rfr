{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to json_traj/traj_fix_dist_2024-01-28.json\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'careems_data/'\n",
    "output_dir = 'clean_data/json_traj/'\n",
    "\n",
    "# Get a list of all input files in the directory\n",
    "input_files = [f for f in os.listdir(input_dir) if f.startswith('pooling_pings_') and f.endswith('.csv')]\n",
    "\n",
    "#extract the date from filename\n",
    "def extract_date_from_filename(filename):\n",
    "    #regular expression to extract the date in the format YYYY-MM-DD\n",
    "    match = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", filename)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(0)  #return extracted date\n",
    "    else:\n",
    "        raise ValueError(\"Date not found in filename. Expected format: trajectories-YYYY-MM-DD.csv\")\n",
    "\n",
    "#get day of the week from date string\n",
    "def day_of_week(date_str):\n",
    "\n",
    "    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "    #get day of the week (Monday is 0, Sunday is 6)\n",
    "    day_index = date.weekday()\n",
    "\n",
    "    days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "    #get day of the week from the index\n",
    "    day_name = days[day_index]\n",
    "\n",
    "     #get day of the month (from 0 to 30)\n",
    "    day_of_month = date.day - 1  \n",
    "\n",
    "    return day_index, day_name, day_of_month\n",
    "\n",
    "#get time ID (minute of the day from 0 to 1439)\n",
    "def time_id_from_timestamp(timestamp_str):\n",
    "    time = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    total_minutes = time.hour * 60 + time.minute\n",
    "\n",
    "    return total_minutes\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    #convert to datetime\n",
    "    df['location_read_at'] = pd.to_datetime(df['location_read_at'])\n",
    "\n",
    "    #calculate distance between two points\n",
    "    def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "        return geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "\n",
    "    #calculate time difference in seconds\n",
    "    def calculate_time_difference(time1, time2):\n",
    "        return (time2 - time1).total_seconds()\n",
    "\n",
    "    trip_data = []\n",
    "\n",
    "    null_booking_id = '9b2d5b4678781e53038e91ea5324530a03f27dc1d0e5f6c9bc9d493a23be9de0'\n",
    "    filtered_df = df[df['hash_booking_id'] != null_booking_id]  #filter out the null booking id\n",
    "    grouped = filtered_df.groupby('hash_booking_id')\n",
    "\n",
    "    for booking_id, group in grouped:\n",
    "        #sort pings by timestamp\n",
    "        group = group.sort_values(by='location_read_at')\n",
    "\n",
    "        driver_id = group['hash_driver_id'].iloc[-1]\n",
    "\n",
    "        #first instance of driver id to track switches\n",
    "        first_instance = group[group['hash_driver_id'] == driver_id].iloc[0]\n",
    "\n",
    "        time_id = first_instance['location_read_at']\n",
    "\n",
    "        #filter out pings before switch\n",
    "        valid_group = group[group['location_read_at'] >= time_id]\n",
    "\n",
    "        lngs = valid_group['longitude'].tolist()\n",
    "        lats = valid_group['latitude'].tolist()\n",
    "\n",
    "        #dist gaps\n",
    "        dist_gaps = [0]\n",
    "        time_gaps = [0]\n",
    "        prev_lat = lats[0]\n",
    "        prev_lng = lngs[0]\n",
    "        prev_time = time_id\n",
    "        cum_dist = 0\n",
    "\n",
    "        #total distance\n",
    "        for lat, lng in zip(lats[1:], lngs[1:]):\n",
    "            dist = calculate_distance(prev_lat, prev_lng, lat, lng)\n",
    "            cum_dist += dist\n",
    "            dist_gaps.append(cum_dist)\n",
    "            prev_lat = lat\n",
    "            prev_lng = lng\n",
    "\n",
    "        total_dist = cum_dist\n",
    "\n",
    "        #time gaps\n",
    "        time_gaps = [(t - time_id).total_seconds() for t in valid_group['location_read_at']]\n",
    "\n",
    "        #total time\n",
    "        total_time = calculate_time_difference(valid_group['location_read_at'].iloc[0], valid_group['location_read_at'].iloc[-1])\n",
    "\n",
    "        trip_data.append([booking_id, driver_id, time_id, lngs, lats, total_dist, total_time, time_gaps, dist_gaps])\n",
    "\n",
    "    output_df = pd.DataFrame(trip_data, columns=['booking_id', 'driver_id', 'time_id', 'lngs', 'lats', 'dist', 'time', 'time_gap', 'dist_gap'])\n",
    "\n",
    "    # Convert DataFrame to list of dicts for JSON processing\n",
    "    trip_data_dicts = output_df.to_dict('records')\n",
    "\n",
    "    # Create JSON objects\n",
    "    json_data = []\n",
    "    date_str = extract_date_from_filename(input_file)\n",
    "\n",
    "    for row in trip_data_dicts:\n",
    "        # Day of the week\n",
    "        week_id, name, date_id = day_of_week(date_str)\n",
    "        # Time ID is minute of day\n",
    "        time_id = time_id_from_timestamp(str(row['time_id']))\n",
    "\n",
    "        new_dict = {\n",
    "            'trip_id': row['booking_id'],\n",
    "            'time_gap': row['time_gap'],\n",
    "            'dist': float(row['dist']),\n",
    "            'lats': row['lats'],\n",
    "            'driverID': row['driver_id'],\n",
    "            'weekID': week_id,\n",
    "            'timeID': time_id,\n",
    "            'dateID': date_id,\n",
    "            'time': float(row['time']),\n",
    "            'lngs': row['lngs'],\n",
    "            'dist_gap': row['dist_gap']\n",
    "        }\n",
    "        json_str = json.dumps(new_dict, separators=(',', ':'))  # Convert to JSON string\n",
    "        json_data.append(json_str)\n",
    "\n",
    "    # Write JSON data to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as output_file:\n",
    "        for entry in json_data:\n",
    "            output_file.write(entry + '\\n')\n",
    "            output_file.flush()\n",
    "\n",
    "# Process each input file\n",
    "for input_file in input_files:\n",
    "    date_str = input_file.split('_')[-1].split('.')[0]  # Extract date from file name\n",
    "    output_file = os.path.join(output_dir, f'traj_fix_dist_{date_str}.json')\n",
    "    process_file(os.path.join(input_dir, input_file), output_file)\n",
    "    print(f\"Data has been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract clean trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and cleaned data for date: 2024-01-22\n",
      "Processed and cleaned data for date: 2024-01-11\n",
      "Processed and cleaned data for date: 2024-01-23\n",
      "Processed and cleaned data for date: 2024-01-01\n",
      "Processed and cleaned data for date: 2024-01-21\n",
      "Processed and cleaned data for date: 2024-01-06\n",
      "Processed and cleaned data for date: 2024-01-12\n",
      "Processed and cleaned data for date: 2024-01-05\n",
      "Processed and cleaned data for date: 2024-01-17\n",
      "Processed and cleaned data for date: 2024-01-24\n",
      "Processed and cleaned data for date: 2024-01-09\n",
      "Processed and cleaned data for date: 2024-01-15\n",
      "Processed and cleaned data for date: 2024-01-13\n",
      "Processed and cleaned data for date: 2024-01-04\n",
      "Processed and cleaned data for date: 2024-01-14\n",
      "Processed and cleaned data for date: 2024-01-16\n",
      "Processed and cleaned data for date: 2024-01-03\n",
      "Processed and cleaned data for date: 2024-01-18\n",
      "Processed and cleaned data for date: 2024-01-08\n",
      "Processed and cleaned data for date: 2024-01-19\n",
      "Processed and cleaned data for date: 2024-01-02\n",
      "Processed and cleaned data for date: 2024-01-20\n",
      "Processed and cleaned data for date: 2024-01-10\n",
      "Processed and cleaned data for date: 2024-01-07\n",
      "Matching process completed. Check the 'trial week/clean_data/' directory for results.\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'clean_data/json_traj/'\n",
    "output_dir = 'clean_data/'\n",
    "clean_file_template = \"clean_{date}.json\"\n",
    "\n",
    "# Load pooling data\n",
    "poolin_dir = 'careems_data/'\n",
    "dp = []\n",
    "\n",
    "# Loop through all pooling files \n",
    "for file_name in os.listdir(poolin_dir):\n",
    "    if file_name.startswith('anon_pooling') and file_name.endswith('.csv'):  # Check file name and extension\n",
    "        file_path = os.path.join(poolin_dir, file_name)\n",
    "        df = pd.read_csv(file_path)  \n",
    "        dp.append(df)  \n",
    "pooling_data = pd.concat(dp, ignore_index=True)\n",
    "\n",
    "\n",
    "for j in os.listdir(input_dir):\n",
    "    date_str = j.split('_')[1].split('.')[0]  # Extract date from file name\n",
    "    filtered_pooling_data = pooling_data[pooling_data['day'] == date_str]\n",
    "    \n",
    "    if filtered_pooling_data.empty:\n",
    "        print(f\"No pooling data for date: {date_str}\")\n",
    "        continue\n",
    "\n",
    "    input_file_path = os.path.join(input_dir, j)\n",
    "    clean_file_path = os.path.join(output_dir, clean_file_template.format(date=date_str))\n",
    "\n",
    "    with open(input_file_path, \"r\") as json_traj_file:\n",
    "        new_data = [json.loads(line) for line in json_traj_file]  # Read each line as a JSON object\n",
    "\n",
    "    with open(clean_file_path, 'w', encoding='utf-8') as clean_file:\n",
    "        for entry in new_data:\n",
    "            entry_time = entry[\"time\"]\n",
    "            entry_trip_id = entry[\"trip_id\"]\n",
    "            entry_driver_id = entry[\"driverID\"]\n",
    "\n",
    "            for _, row in filtered_pooling_data.iterrows():\n",
    "                pool_time = float(row[\"captain_engagement_time\"] * 60)\n",
    "                pool_trip_id = row[\"booking_id\"]\n",
    "                pool_driver_id = row[\"captain_id\"]\n",
    "                time_diff = pool_time - entry_time\n",
    "\n",
    "                # Adding time difference to entry\n",
    "                entry_with_time_diff = entry.copy()\n",
    "                entry_with_time_diff[\"time_diff\"] = time_diff\n",
    "\n",
    "                # Writing good trips to clean file (same trip and driver id and <= 5 sec time diff)\n",
    "                if entry_trip_id == pool_trip_id and entry_driver_id == pool_driver_id and abs(time_diff) <= 5:\n",
    "                    json.dump(entry_with_time_diff, clean_file)\n",
    "                    clean_file.write(\"\\n\")\n",
    "                    break\n",
    "\n",
    "    print(f\"Processed and cleaned data for date: {date_str}\")\n",
    "\n",
    "print(\"Matching process completed. Check the 'trial week/clean_data/' directory for results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Splitting Trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "from shapely.ops import unary_union\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merchant data\n",
    "merchant_file_path = 'careems_data/order_merchant_id_anon.parquet'\n",
    "merchants_df = pd.read_parquet(merchant_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pooling data\n",
    "directory = 'careems_data/'\n",
    "dp = []\n",
    "\n",
    "# Loop through all pooling files \n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.startswith('anon_pooling') and file_name.endswith('.csv'):  # Check file name and extension\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        df = pd.read_csv(file_path)  \n",
    "        dp.append(df)  \n",
    "\n",
    "\n",
    "df_pooling = pd.concat(dp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean data and drop time_diff column\n",
    "directory = 'clean_data/'\n",
    "dp = []\n",
    "\n",
    "# Loop through all clean data files \n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.startswith('clean_2024-') and file_name.endswith('.json'):  # Check file name and extension\n",
    "        month = int(file_name.split('-')[1])  # Extract the month and convert to int\n",
    "        file_path = os.path.join(directory, file_name)  \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:  \n",
    "                entry = json.loads(line)  \n",
    "                entry['month'] = month  # Add month to the data\n",
    "                dp.append(entry) \n",
    "\n",
    "\n",
    "df = pd.DataFrame(dp)\n",
    "\n",
    "if 'time_diff' in df.columns:\n",
    "    df.drop('time_diff', axis=1, inplace=True)\n",
    "\n",
    "df = df.rename(columns={'time': 'trip_time'})\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left join merchants on the pings df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Merge merchants_df with df_pooling on 'order_id'\n",
    "merged_pooling = df_pooling.merge(merchants_df[['order_id', 'merchant_id']], on='order_id', how='left')\n",
    "\n",
    "# Merge with df on 'trip_id' \n",
    "df = df.merge(merged_pooling[['booking_id', 'merchant_id']], left_on='trip_id', right_on='booking_id', how='left')\n",
    "\n",
    "df = df.drop(columns=['booking_id'])\n",
    "\n",
    "# Check how many 'merchant_id' values are null \n",
    "print(df['merchant_id'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Stationarity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all stationary intervals over each trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_stationary_intervals(df):\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = np.array(row['lats'])\n",
    "        lngs = np.array(row['lngs'])\n",
    "        time_gap = np.array(row['time_gap'])\n",
    "        \n",
    "        # Compute differences between consecutive points directly from lat/lng arrays\n",
    "        lat_diff = np.abs(np.diff(lats))\n",
    "        lng_diff = np.abs(np.diff(lngs))\n",
    "\n",
    "        # Identify stationary intervals\n",
    "        stationary_indices = np.where((lat_diff == 0) & (lng_diff == 0))[0]\n",
    "        \n",
    "        # Group consecutive stationary indices into intervals\n",
    "        if len(stationary_indices) > 0:\n",
    "            start_idx = stationary_indices[0]\n",
    "            for i in range(1, len(stationary_indices)):\n",
    "                # If the current index is not consecutive, close the interval\n",
    "                if stationary_indices[i] != stationary_indices[i - 1] + 1:\n",
    "                    end_idx = stationary_indices[i - 1]\n",
    "                    # Save the interval only if it has more than one index\n",
    "                    if end_idx > start_idx:\n",
    "                        interval = {\n",
    "                            \"trip_id\": trip_id,\n",
    "                            \"start_idx\": int(start_idx),\n",
    "                            \"end_idx\": int(end_idx)+1,\n",
    "                            \"start_lat\": float(lats[start_idx]),\n",
    "                            \"start_lng\": float(lngs[start_idx]),\n",
    "                            \"end_lat\": float(lats[end_idx+1]),\n",
    "                            \"end_lng\": float(lngs[end_idx+1]),\n",
    "                            \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                        }\n",
    "                        results.append(interval)\n",
    "                    # Start a new interval\n",
    "                    start_idx = stationary_indices[i]\n",
    "            # Add the last interval if it has more than one index\n",
    "            end_idx = stationary_indices[-1]\n",
    "            if end_idx > start_idx:\n",
    "                interval = {\n",
    "                    \"trip_id\": trip_id,\n",
    "                    \"start_idx\": int(start_idx),\n",
    "                    \"end_idx\": int(end_idx)+1,\n",
    "                    \"start_lat\": float(lats[start_idx]),\n",
    "                    \"start_lng\": float(lngs[start_idx]),\n",
    "                    \"end_lat\": float(lats[end_idx+1]),\n",
    "                    \"end_lng\": float(lngs[end_idx+1]),\n",
    "                    \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                }\n",
    "                results.append(interval)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "stationary_df_pre_truncation = detect_stationary_intervals(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate trips where a stationary interval exists at the end of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def truncate_trips_if_stationary(df, stationary_df):\n",
    "    truncated_trips = []\n",
    "\n",
    "    # Iterate over each trip in the df\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = row['lats']\n",
    "        lngs = row['lngs']\n",
    "        time_gap = row['time_gap']\n",
    "        dist_gap = row['dist_gap']\n",
    "        dist = row['dist']\n",
    "        trip_time = row['trip_time']\n",
    "        month = row['month']\n",
    "\n",
    "        # Get the last coordinates of the trip\n",
    "        last_lat, last_lng = lats[-1], lngs[-1]\n",
    "\n",
    "        # Find the corresponding stationary intervals for this trip\n",
    "        intervals = stationary_df[stationary_df['trip_id'] == trip_id]\n",
    "\n",
    "        if not intervals.empty:\n",
    "            # Get the last stationary interval for this trip\n",
    "            last_interval = intervals.iloc[-1]\n",
    "\n",
    "            interval_lat, interval_lng = last_interval['end_lat'], last_interval['end_lng']\n",
    "            time_elapsed = last_interval['time_elapsed']\n",
    "\n",
    "            # Check if the last stationary interval's coordinates match the last trip coordinates\n",
    "            if (last_lat == interval_lat and last_lng == interval_lng) and time_elapsed > 10:\n",
    "                # Keep only the first part of the stationary segment\n",
    "                truncated_trip = {\n",
    "                    'trip_id': trip_id,\n",
    "                    'lats': lats[:last_interval['start_idx'] + 1],\n",
    "                    'lngs': lngs[:last_interval['start_idx'] + 1],\n",
    "                    'time_gap': time_gap[:last_interval['start_idx'] + 1],\n",
    "                    'dist_gap': dist_gap[:last_interval['start_idx'] + 1],\n",
    "                    'dist': dist,  # Keep the original distance\n",
    "                    'trip_time': time_gap[last_interval['start_idx']],  # Adjusted trip time\n",
    "                    'driverID': row['driverID'],\n",
    "                    'weekID': row['weekID'],\n",
    "                    'timeID': row['timeID'],\n",
    "                    'dateID': row['dateID'],\n",
    "                    'merchant_id': row['merchant_id'],\n",
    "                    'month': month\n",
    "                }\n",
    "                truncated_trips.append(truncated_trip)\n",
    "            else:\n",
    "                # If no truncation is needed, keep the original trip\n",
    "                truncated_trips.append(row.to_dict())\n",
    "        else:\n",
    "            # If no stationary intervals exist, keep the original trip\n",
    "            truncated_trips.append(row.to_dict())\n",
    "\n",
    "    # Convert the list of truncated trips back into a df\n",
    "    truncated_df = pd.DataFrame(truncated_trips)\n",
    "\n",
    "    # Ensure all columns match the original schema\n",
    "    for column in df.columns:\n",
    "        if column not in truncated_df.columns:\n",
    "            truncated_df[column] = None  # Add missing columns with default None values\n",
    "\n",
    "    # Reorder columns to match the original DataFrame\n",
    "    truncated_df = truncated_df[df.columns]\n",
    "\n",
    "    return truncated_df\n",
    "\n",
    "truncated_df = truncate_trips_if_stationary(df, stationary_df_pre_truncation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find new stationary intervals post truncation and longest interval for each trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationary intervals saved to stationary_intervals_all_trips_truncated.csv\n"
     ]
    }
   ],
   "source": [
    "def detect_stationary_intervals(df):\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = np.array(row['lats'])\n",
    "        lngs = np.array(row['lngs'])\n",
    "        time_gap = np.array(row['time_gap'])\n",
    "        \n",
    "        # Compute differences between consecutive points directly from lat/lng arrays\n",
    "        lat_diff = np.abs(np.diff(lats))\n",
    "        lng_diff = np.abs(np.diff(lngs))\n",
    "\n",
    "        # Identify stationary intervals \n",
    "        stationary_indices = np.where((lat_diff == 0) & (lng_diff == 0))[0]\n",
    "        \n",
    "        # Group consecutive stationary indices into intervals\n",
    "        if len(stationary_indices) > 0:\n",
    "            start_idx = stationary_indices[0]\n",
    "            for i in range(1, len(stationary_indices)):\n",
    "                # If the current index is not consecutive, close the interval\n",
    "                if stationary_indices[i] != stationary_indices[i - 1] + 1:\n",
    "                    end_idx = stationary_indices[i - 1]\n",
    "                    # Save the interval only if it has more than one index\n",
    "                    if end_idx > start_idx:\n",
    "                        interval = {\n",
    "                            \"trip_id\": trip_id,\n",
    "                            \"start_idx\": int(start_idx),\n",
    "                            \"end_idx\": int(end_idx)+1,\n",
    "                            \"start_lat\": float(lats[start_idx]),\n",
    "                            \"start_lng\": float(lngs[start_idx]),\n",
    "                            \"end_lat\": float(lats[end_idx+1]),\n",
    "                            \"end_lng\": float(lngs[end_idx+1]),\n",
    "                            \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                        }\n",
    "                        results.append(interval)\n",
    "                    # Start a new interval\n",
    "                    start_idx = stationary_indices[i]\n",
    "            # Add the last interval if it has more than one index\n",
    "            end_idx = stationary_indices[-1]\n",
    "            if end_idx > start_idx:\n",
    "                interval = {\n",
    "                    \"trip_id\": trip_id,\n",
    "                    \"start_idx\": int(start_idx),\n",
    "                    \"end_idx\": int(end_idx)+1,\n",
    "                    \"start_lat\": float(lats[start_idx]),\n",
    "                    \"start_lng\": float(lngs[start_idx]),\n",
    "                    \"end_lat\": float(lats[end_idx+1]),\n",
    "                    \"end_lng\": float(lngs[end_idx+1]),\n",
    "                    \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                }\n",
    "                results.append(interval)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "stationary_df = detect_stationary_intervals(truncated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=truncated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_longest_stationary_interval(stationary_df):\n",
    "    # Find the longest stationary interval for each trip_id based on time_elapsed\n",
    "    longest_intervals = stationary_df.loc[stationary_df.groupby('trip_id')['time_elapsed'].idxmax()]\n",
    "\n",
    "    longest_intervals.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return longest_intervals\n",
    "\n",
    "longest_intervals_df = extract_longest_stationary_interval(stationary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combined df with pooling coordinates and longest stationary interval coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined(longest_intervals_df, pooling_df):\n",
    "    \n",
    "    # Rename 'booking_id' to 'trip_id' in pooling_df for consistency\n",
    "    pooling_df = pooling_df.rename(columns={'booking_id': 'trip_id'})\n",
    "    \n",
    "    combined_df = (\n",
    "        longest_intervals_df[['trip_id', 'start_lat', 'start_lng']]\n",
    "        .merge(pooling_df[['trip_id', 'pickup_latitude', 'pickup_longitude']], on='trip_id', how='inner')\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity in the combined data\n",
    "    combined_df = combined_df.rename(columns={\n",
    "        'start_lat': 'stationary_df_lat',\n",
    "        'start_lng': 'stationary_df_lng',\n",
    "        'pickup_latitude': 'pooling_df_lat',\n",
    "        'pickup_longitude': 'pooling_df_lng'\n",
    "    })   \n",
    "    return combined_df\n",
    "\n",
    "combined_df = create_combined(longest_intervals_df, df_pooling)\n",
    "\n",
    "combined_df = combined_df.merge(df[['trip_id', 'merchant_id']], on='trip_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update incorrect pooling coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistent groups saved to merchant_lat_lng_mismatch.csv\n",
      "Number of inconsistent groups: 987\n"
     ]
    }
   ],
   "source": [
    "def check_inconsistent_groups_with_tolerance_fix(combined_df):\n",
    "    \n",
    "    # Lists to store inconsistent groups and detailed non-unique counts\n",
    "    inconsistent_groups = []\n",
    "    nonunique_info = []\n",
    "\n",
    "    for merchant_id, group in combined_df.groupby('merchant_id'):\n",
    "        # Extract latitude and longitude values for the group\n",
    "        lat_values = group['pooling_df_lat'].values\n",
    "        lng_values = group['pooling_df_lng'].values\n",
    "\n",
    "        # Count occurrences of each latitude and longitude\n",
    "        lat_counts = pd.Series(lat_values).value_counts()\n",
    "        lng_counts = pd.Series(lng_values).value_counts()\n",
    "\n",
    "        # Identify the majority (most common) latitude and longitude\n",
    "        majority_lat = lat_counts.idxmax()\n",
    "        majority_lng = lng_counts.idxmax()\n",
    "\n",
    "        # Filter out the non-majority coordinates\n",
    "        non_majority_lats = lat_counts[lat_counts.index != majority_lat]\n",
    "        non_majority_lngs = lng_counts[lng_counts.index != majority_lng]\n",
    "\n",
    "        # If there are non-majority coordinates, store the information\n",
    "        if not non_majority_lats.empty or not non_majority_lngs.empty:\n",
    "            inconsistent_groups.append(group)\n",
    "\n",
    "            # Store the non-unique information for this merchant_id\n",
    "            nonunique_info.append({\n",
    "                'merchant_id': merchant_id,\n",
    "                'non_majority_lats': non_majority_lats.to_dict(),  # Non-majority latitudes and their counts\n",
    "                'non_majority_lngs': non_majority_lngs.to_dict(),  # Non-majority longitudes and their counts\n",
    "                'group_size': len(group)\n",
    "            })\n",
    "\n",
    "    # Combine all inconsistent groups into a single DataFrame\n",
    "    if inconsistent_groups:\n",
    "        print(f\"Number of inconsistent groups: {len(inconsistent_groups)}\")\n",
    "        non_majority_df = pd.DataFrame(nonunique_info)\n",
    "    else:\n",
    "        print(\"No inconsistencies found.\")\n",
    "\n",
    "    return non_majority_df\n",
    "\n",
    "non_majority_df = check_inconsistent_groups_with_tolerance_fix(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected 531 trip coordinates.\n"
     ]
    }
   ],
   "source": [
    "# Change incorrect pooling coordinates in combined_df to the majority coordinates for each merchant\n",
    "def correct_pooling_coordinates_with_nonmajority(non_majority_df, combined_df):\n",
    "\n",
    "    # Iterate over each merchant in the non-majority df\n",
    "    for _, row in non_majority_df.iterrows():\n",
    "        merchant_id = row['merchant_id']\n",
    "\n",
    "        # Get the majority latitude and longitude for this merchant from combined_df\n",
    "        majority_lat = combined_df[combined_df['merchant_id'] == merchant_id]['pooling_df_lat'].mode()[0]\n",
    "        majority_lng = combined_df[combined_df['merchant_id'] == merchant_id]['pooling_df_lng'].mode()[0]\n",
    "\n",
    "        # Find the trip_ids with non-majority coordinates\n",
    "        merchant_group = combined_df[combined_df['merchant_id'] == merchant_id]\n",
    "        incorrect_trips = merchant_group[\n",
    "            (~np.isclose(merchant_group['pooling_df_lat'], majority_lat, atol=0.001)) |\n",
    "            (~np.isclose(merchant_group['pooling_df_lng'], majority_lng, atol=0.001))\n",
    "        ]\n",
    "\n",
    "        # Update the coordinates in combined_df for the incorrect trips\n",
    "        for trip_id in incorrect_trips['trip_id']:\n",
    "            combined_df.loc[combined_df['booking_id'] == trip_id, ['pooling_df_lat', 'pooling_df_lat']] = [\n",
    "                majority_lat, majority_lng\n",
    "            ]\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "combined_df = correct_pooling_coordinates_with_nonmajority(non_majority_df, combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract non-matching coordinates and drop them from df and longest stationary df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadab\\AppData\\Local\\Temp\\ipykernel_26592\\226864586.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_matching_df['distance_km'] = haversine(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-matching coordinate entries saved to non_matching_coordinates.csv\n",
      "trip_id              10071\n",
      "stationary_df_lat    10071\n",
      "stationary_df_lng    10071\n",
      "pooling_df_lat       10071\n",
      "pooling_df_lng       10071\n",
      "merchant_id          10071\n",
      "distance_km          10071\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "\n",
    "    # Compute differences\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Apply Haversine formula\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "def extract_non_matching_coordinates(combined_df):\n",
    "    # Identify rows where the stationary coordinates do not match pooling coordinates\n",
    "    non_matching_df = combined_df[\n",
    "        (combined_df['stationary_df_lat'] != combined_df['pooling_df_lat']) |\n",
    "        (combined_df['stationary_df_lng'] != combined_df['pooling_df_lng'])\n",
    "    ]\n",
    "\n",
    "    # calculate the distance between the coordinates\n",
    "    non_matching_df['distance_km'] = haversine(\n",
    "        non_matching_df['stationary_df_lat'],\n",
    "        non_matching_df['stationary_df_lng'],\n",
    "        non_matching_df['pooling_df_lat'],\n",
    "        non_matching_df['pooling_df_lng']\n",
    "    )\n",
    "\n",
    "    return non_matching_df\n",
    "\n",
    "non_matching_entries = extract_non_matching_coordinates(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrames saved.\n"
     ]
    }
   ],
   "source": [
    "# Drop non-matching trips from `df` and `longest_intervals_df`\n",
    "def drop_non_matching_trips(df, longest_intervals_df, non_matching_entries):\n",
    "    # Extract the trip IDs from non-matching entries\n",
    "    non_matching_trip_ids = non_matching_entries['trip_id'].unique()\n",
    "\n",
    "    # Drop these trips from both dfs\n",
    "    df_filtered = df[~df['trip_id'].isin(non_matching_trip_ids)]\n",
    "    longest_intervals_filtered = longest_intervals_df[~longest_intervals_df['trip_id'].isin(non_matching_trip_ids)]\n",
    "\n",
    "    return df_filtered, longest_intervals_filtered\n",
    "\n",
    "\n",
    "df, longest_intervals_df = drop_non_matching_trips(df, longest_intervals_df, non_matching_entries)\n",
    "print(\"Filtered DataFrames saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Splitting trips into 3 segments (Driver-to-Merchant, Wait-Time-at-Merchant, Merchant-to-Customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split trips based on stationary intervals matching coordinates from pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No interval found for trip_id: 25049734158c2944703546bdc2cbe5740170230779e858db275cb6512aaa0047\n",
      "No interval found for trip_id: 4ad4bdf931291e3f2176bef563091893e0bde118f28a2ef5b172ece38949ed38\n",
      "No interval found for trip_id: 5057cb61e831386cd034d24462a4f6ecebecb3606bc2d39d4c991fd04b56e413\n",
      "No interval found for trip_id: 620950188eebce63b6bf5be22cb280dbb8d75b5cef8904b6de30bb892d2a1c3a\n",
      "No interval found for trip_id: 64186edda7362e34c7598fe8af53c01cde6071b3d932380eaa30299419712632\n",
      "No interval found for trip_id: 07a2f3cdf28096c27c4c2b81bcfafe30a96095097f6e96e79878f714a9ec648c\n",
      "No interval found for trip_id: 6299a27e7d370d6904a05073fdd554e0efbf09f0d9458d7e0f677e75644c2e34\n",
      "No interval found for trip_id: 65d99e964c50a5eed233fe9a7864114c1d91cf6b6ba542bbf4906a998d4df27f\n",
      "No interval found for trip_id: 7d71d55894365ccad46d7e7c533997c58e200cc235919b0c817944af8b5226a1\n",
      "No interval found for trip_id: 01ff05c850fb030d8174a48d70a7327747dd0177030bc5cad10f5c9d25b95b33\n",
      "No interval found for trip_id: ec0286dc18471dfaf418922d48c55f7ec72f1e528954b80fc707705137fb5a21\n",
      "No interval found for trip_id: f01f5f9ddc73c41afad46682bccac8a38ccfad813cd19c7bacbde98ae8b96cff\n",
      "No interval found for trip_id: f82b6c7a45db4fd93e76d73881343d2aeeec3fefb876401a4b9380c9ad3cf37d\n",
      "No interval found for trip_id: 5dc0e366867ddce4caf3a1861cd46b216165cbb1c513cd796e53942b12c9eb1a\n",
      "No interval found for trip_id: b61a3a9ada58f81859560718f1697a84aa9543ac6be94e048b54178ea29dac66\n",
      "No interval found for trip_id: bad44c8533c0435d81119b807997f4836a6648da594f5d4da2ece47bc1bed460\n",
      "No interval found for trip_id: e0e7a3c5ceb935f13325f33aada255dc33fb8e7d343dcd21607e2f8606378005\n",
      "No interval found for trip_id: f9a051e1debe7d4dda61a109a99262a6ae77d6cc261a822badd05242f94bc307\n",
      "No interval found for trip_id: 35eb23a5ef4a2ea073a83d60f6c5bfd77111f100688cbf2a2ed9dddc9a90aeba\n",
      "No interval found for trip_id: 553a5a3eac65d331a068a8a60c29ce6c3ce4bc4cff4c63542f2d45cdc62bd55f\n",
      "No interval found for trip_id: 64e730589bbba0cb6c3f5b326825303c12de74cce5039304b8d75c4130834950\n",
      "No interval found for trip_id: 95211438a9e97d1e5ca9185f0a6b2c814e399dc323853b0fb85d0d6918d12ca6\n",
      "No interval found for trip_id: 08d42691a20f566e666116e678f3c56651624a768b7a43092a5245acbbc71ace\n",
      "No interval found for trip_id: 4a364cf873fe027a333ee1ad5be259751515200bca58d5352103e0c7b3306aec\n",
      "No interval found for trip_id: 756619c50b17ee83526dda6c0ee4ead83cf50cdb8b76718630c9f10aa6e09119\n",
      "No interval found for trip_id: a8178a79f9a2b059f4e951777992d4e6c36e3782a5767e4010f0831a46e36d6a\n",
      "No interval found for trip_id: e0933d83f8c0492912d0ba4418433f3114df0d6ae9c320ae2d62cd2099262b8f\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def adjust_gaps_with_original_pattern(original_gaps, new_gaps):\n",
    "    \"\"\"\n",
    "    Adjust new gaps based on the pattern of change in the original gaps.\n",
    "    The new gaps will start from 0 but follow the same incremental differences.\n",
    "    \"\"\"\n",
    "    if len(original_gaps) <= 1 or len(new_gaps) == 0:\n",
    "        return new_gaps  # No meaningful adjustment needed\n",
    "\n",
    "    # Calculate the incremental changes (differences) in the original gaps\n",
    "    original_diffs = np.diff(original_gaps)\n",
    "\n",
    "    # Start the new gaps from 0\n",
    "    adjusted_gaps = [0]\n",
    "\n",
    "    # Apply the original differences to the new gaps\n",
    "    for i in range(1, len(new_gaps)):\n",
    "        diff = original_diffs[(i - 1) % len(original_diffs)]\n",
    "        adjusted_gaps.append(adjusted_gaps[-1] + diff)\n",
    "\n",
    "    return adjusted_gaps\n",
    "\n",
    "def segment_trips(df, longest_intervals_df):\n",
    "    road_list = []\n",
    "    second_segment_list = []\n",
    "\n",
    "    # Iterate over each entry in the df dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        interval = longest_intervals_df[longest_intervals_df['trip_id'] == trip_id]\n",
    "        lats = row['lats']\n",
    "        lngs = row['lngs']\n",
    "        time_gap = row['time_gap']\n",
    "        dist_gap = row['dist_gap']\n",
    "        month = row['month']\n",
    "\n",
    "        if not interval.empty:\n",
    "            start_idx = interval['start_idx'].values[0]\n",
    "            end_idx = interval['end_idx'].values[0]\n",
    "\n",
    "            # First segment\n",
    "            first_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': time_gap[:start_idx],\n",
    "                'dist': dist_gap[start_idx - 1] if start_idx > 0 else 0,\n",
    "                'trip_time': time_gap[start_idx - 1] if start_idx > 0 else 0,\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID'],\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': dist_gap[:start_idx],\n",
    "                'lats': lats[:start_idx],\n",
    "                'lngs': lngs[:start_idx],\n",
    "                'month': month,\n",
    "                'time_offset': 0,\n",
    "                'segmentID': 1\n",
    "            }\n",
    "\n",
    "            # Second segment (stationary)\n",
    "            stationary_time_gap = time_gap[start_idx:end_idx + 1]\n",
    "            stationary_dist_gap = dist_gap[start_idx:end_idx + 1]\n",
    "\n",
    "            stationary_time_gap = [time - stationary_time_gap[0] for time in stationary_time_gap]\n",
    "            stationary_dist_gap = [dist - stationary_dist_gap[0] for dist in stationary_dist_gap]\n",
    "\n",
    "            second_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': stationary_time_gap,\n",
    "                'dist': stationary_dist_gap[-1] if len(stationary_dist_gap) > 0 else 0,\n",
    "                'trip_time': stationary_time_gap[-1] if len(stationary_time_gap) > 0 else 0,\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID'],\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': stationary_dist_gap,\n",
    "                'lats': lats[start_idx:end_idx + 1],\n",
    "                'lngs': lngs[start_idx:end_idx + 1],\n",
    "                'month': month,\n",
    "                'time_offset': time_gap[start_idx],\n",
    "                'merchant': row['merchant_id']\n",
    "            }\n",
    "\n",
    "            # Third segment (after stationary)\n",
    "            new_time_gap = time_gap[end_idx + 1:]\n",
    "            new_dist_gap = dist_gap[end_idx + 1:]\n",
    "\n",
    "            # Adjust the gaps using the pattern from the original trip\n",
    "            adjusted_time_gap = adjust_gaps_with_original_pattern(time_gap[end_idx + 1:], new_time_gap)\n",
    "            adjusted_dist_gap = adjust_gaps_with_original_pattern(dist_gap[end_idx + 1:], new_dist_gap)\n",
    "            time_offset3 = time_gap[end_idx + 1] if len(time_gap) > end_idx + 1 else 0\n",
    "\n",
    "            third_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': adjusted_time_gap,\n",
    "                'dist': adjusted_dist_gap[-1] if len(adjusted_dist_gap) > 0 else 0,\n",
    "                'trip_time': adjusted_time_gap[-1] if len(adjusted_time_gap) > 0 else 0,\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID']+round((time_offset3/60),1),\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': adjusted_dist_gap,\n",
    "                'lats': lats[end_idx + 1:],\n",
    "                'lngs': lngs[end_idx + 1:],\n",
    "                'month': month,\n",
    "                'time_offset': time_offset3,\n",
    "                'segmentID': 3\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No interval found for trip_id: {trip_id}\")\n",
    "\n",
    "        # Add segments to their respective lists\n",
    "        if len(first_segment['time_gap']) > 1:\n",
    "            road_list.append(first_segment)\n",
    "\n",
    "        if len(third_segment['time_gap']) > 1:\n",
    "            road_list.append(third_segment)\n",
    "\n",
    "        if len(second_segment['time_gap']) > 1:\n",
    "            second_segment_list.append(second_segment)\n",
    "\n",
    "    # Convert the lists to DataFrames\n",
    "    road_df = pd.DataFrame(road_list)\n",
    "    second_segment_df = pd.DataFrame(second_segment_list)\n",
    "\n",
    "    return road_df, second_segment_df\n",
    "\n",
    "road_df, second_segment_df = segment_trips(df, longest_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_25.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_25.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_26.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_26.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_27.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_27.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_28.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_28.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_29.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_29.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_30.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_30.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_31.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_31.json'\n",
      "Index(['trip_id', 'time_gap', 'dist', 'trip_time', 'driverID', 'weekID',\n",
      "       'timeID', 'dateID', 'dist_gap', 'lats', 'lngs', 'time_offset',\n",
      "       'segmentID'],\n",
      "      dtype='object')\n",
      "Index(['trip_id', 'time_gap', 'dist', 'trip_time', 'driverID', 'weekID',\n",
      "       'timeID', 'dateID', 'dist_gap', 'lats', 'lngs', 'time_offset',\n",
      "       'merchant'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Remove trips with single 3rd segment occurences\n",
    "single_occurrence_trips = road_df['trip_id'].value_counts()\n",
    "single_occurrence_trips = single_occurrence_trips[single_occurrence_trips == 1].index\n",
    "\n",
    "filtered_single_trips_df = road_df[road_df['trip_id'].isin(single_occurrence_trips)]\n",
    "\n",
    "filtered_single_trips_segment3_df = filtered_single_trips_df[filtered_single_trips_df['segmentID'] == 1]\n",
    "\n",
    "# Remove these trips from road_df\n",
    "road_df = road_df[~road_df['trip_id'].isin(filtered_single_trips_segment3_df['trip_id'])]\n",
    "\n",
    "# Extract unique combinations of dateID and month from road_df\n",
    "unique_date_month_combinations = road_df[['dateID', 'month']].drop_duplicates()\n",
    "\n",
    "# Iterate over each unique combination\n",
    "for _, row in unique_date_month_combinations.iterrows():\n",
    "    date_id = row['dateID']\n",
    "    month = row['month']\n",
    "    \n",
    "    # Filter road_df and second_segment_df for the current dateID and month\n",
    "    road_df_filtered = road_df[(road_df['dateID'] == date_id) & (road_df['month'] == month)]\n",
    "    second_segment_df_filtered = second_segment_df[(second_segment_df['dateID'] == date_id) & (second_segment_df['month'] == month)]\n",
    "    \n",
    "    # Convert to JSON format\n",
    "    road_json = road_df_filtered.to_dict(orient='records')\n",
    "    second_seg_json = second_segment_df_filtered.to_dict(orient='records')\n",
    "\n",
    "    # Generate filenames with month and dateID + 1\n",
    "    road_file_name = f'segmented_trips/Segmented_Trips_0{month}_{date_id + 1}.json'\n",
    "    segment_file_name = f'segmented_trips/Merchants_Segments_0{month}_{date_id + 1}.json'\n",
    "\n",
    "    # Save files\n",
    "    with open(road_file_name, 'w') as file:\n",
    "        for json_obj in road_json:\n",
    "            json.dump(json_obj, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "    with open(segment_file_name, 'w') as file:\n",
    "        for json_obj in second_seg_json:\n",
    "            json.dump(json_obj, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "    print(f\"Segments 1 & 3 saved to '{road_file_name}'\")\n",
    "    print(f\"Segment 2 with wait times saved to '{segment_file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>weekID</th>\n",
       "      <th>timeID</th>\n",
       "      <th>dateID</th>\n",
       "      <th>time_offset</th>\n",
       "      <th>segmentID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.245647</td>\n",
       "      <td>545.493941</td>\n",
       "      <td>2.995945</td>\n",
       "      <td>882.030065</td>\n",
       "      <td>26.887450</td>\n",
       "      <td>462.391729</td>\n",
       "      <td>2.081147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.907593</td>\n",
       "      <td>437.792605</td>\n",
       "      <td>1.946319</td>\n",
       "      <td>280.359040</td>\n",
       "      <td>2.044701</td>\n",
       "      <td>537.200286</td>\n",
       "      <td>0.996710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.883395</td>\n",
       "      <td>208.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>686.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.373664</td>\n",
       "      <td>432.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>904.200000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.662286</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1094.900000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>815.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2872.321048</td>\n",
       "      <td>4980.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1437.400000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>6442.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dist     trip_time        weekID        timeID        dateID  \\\n",
       "count  64611.000000  64611.000000  64611.000000  64611.000000  64611.000000   \n",
       "mean       4.245647    545.493941      2.995945    882.030065     26.887450   \n",
       "std       25.907593    437.792605      1.946319    280.359040      2.044701   \n",
       "min        0.000000      0.000000      0.000000      3.000000     24.000000   \n",
       "25%        0.883395    208.500000      1.000000    686.000000     25.000000   \n",
       "50%        2.373664    432.000000      3.000000    904.200000     27.000000   \n",
       "75%        5.662286    780.000000      5.000000   1094.900000     29.000000   \n",
       "max     2872.321048   4980.000000      6.000000   1437.400000     30.000000   \n",
       "\n",
       "        time_offset     segmentID  \n",
       "count  64611.000000  64611.000000  \n",
       "mean     462.391729      2.081147  \n",
       "std      537.200286      0.996710  \n",
       "min        0.000000      1.000000  \n",
       "25%        0.000000      1.000000  \n",
       "50%      344.000000      3.000000  \n",
       "75%      815.000000      3.000000  \n",
       "max     6442.000000      3.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "road_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>weekID</th>\n",
       "      <th>timeID</th>\n",
       "      <th>dateID</th>\n",
       "      <th>time_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35009.0</td>\n",
       "      <td>35009.000000</td>\n",
       "      <td>35009.000000</td>\n",
       "      <td>35009.000000</td>\n",
       "      <td>35009.000000</td>\n",
       "      <td>35009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>587.805764</td>\n",
       "      <td>2.981176</td>\n",
       "      <td>874.914708</td>\n",
       "      <td>26.907024</td>\n",
       "      <td>254.751550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>392.575488</td>\n",
       "      <td>1.944636</td>\n",
       "      <td>282.744044</td>\n",
       "      <td>2.050928</td>\n",
       "      <td>251.245861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>677.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>498.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>785.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1090.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>358.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5824.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1429.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3415.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dist     trip_time        weekID        timeID        dateID  \\\n",
       "count  35009.0  35009.000000  35009.000000  35009.000000  35009.000000   \n",
       "mean       0.0    587.805764      2.981176    874.914708     26.907024   \n",
       "std        0.0    392.575488      1.944636    282.744044      2.050928   \n",
       "min        0.0     10.000000      0.000000      3.000000     24.000000   \n",
       "25%        0.0    295.000000      1.000000    677.000000     25.000000   \n",
       "50%        0.0    498.000000      3.000000    898.000000     27.000000   \n",
       "75%        0.0    785.000000      5.000000   1090.000000     29.000000   \n",
       "max        0.0   5824.000000      6.000000   1429.000000     30.000000   \n",
       "\n",
       "        time_offset  \n",
       "count  35009.000000  \n",
       "mean     254.751550  \n",
       "std      251.245861  \n",
       "min        0.000000  \n",
       "25%       77.000000  \n",
       "50%      195.000000  \n",
       "75%      358.000000  \n",
       "max     3415.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "second_segment_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
