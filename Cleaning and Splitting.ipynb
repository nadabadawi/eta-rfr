{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to json_traj/traj_fix_dist_2024-01-28.json\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'careems_data/'\n",
    "output_dir = 'clean_data/json_traj/'\n",
    "\n",
    "# Get a list of all input files in the directory\n",
    "input_files = [f for f in os.listdir(input_dir) if f.startswith('pooling_pings_') and f.endswith('.csv')]\n",
    "\n",
    "#extract the date from filename\n",
    "def extract_date_from_filename(filename):\n",
    "    #regular expression to extract the date in the format YYYY-MM-DD\n",
    "    match = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", filename)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(0)  #return extracted date\n",
    "    else:\n",
    "        raise ValueError(\"Date not found in filename. Expected format: trajectories-YYYY-MM-DD.csv\")\n",
    "\n",
    "#get day of the week from date string\n",
    "def day_of_week(date_str):\n",
    "\n",
    "    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "    #get day of the week (Monday is 0, Sunday is 6)\n",
    "    day_index = date.weekday()\n",
    "\n",
    "    days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "    #get day of the week from the index\n",
    "    day_name = days[day_index]\n",
    "\n",
    "     #get day of the month (from 0 to 30)\n",
    "    day_of_month = date.day - 1  \n",
    "\n",
    "    return day_index, day_name, day_of_month\n",
    "\n",
    "#get time ID (minute of the day from 0 to 1439)\n",
    "def time_id_from_timestamp(timestamp_str):\n",
    "    time = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    total_minutes = time.hour * 60 + time.minute\n",
    "\n",
    "    return total_minutes\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    #convert to datetime\n",
    "    df['location_read_at'] = pd.to_datetime(df['location_read_at'])\n",
    "\n",
    "    #calculate distance between two points\n",
    "    def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "        return geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "\n",
    "    #calculate time difference in seconds\n",
    "    def calculate_time_difference(time1, time2):\n",
    "        return (time2 - time1).total_seconds()\n",
    "\n",
    "    trip_data = []\n",
    "\n",
    "    null_booking_id = '9b2d5b4678781e53038e91ea5324530a03f27dc1d0e5f6c9bc9d493a23be9de0'\n",
    "    filtered_df = df[df['hash_booking_id'] != null_booking_id]  #filter out the null booking id\n",
    "    grouped = filtered_df.groupby('hash_booking_id')\n",
    "\n",
    "    for booking_id, group in grouped:\n",
    "        #sort pings by timestamp\n",
    "        group = group.sort_values(by='location_read_at')\n",
    "\n",
    "        driver_id = group['hash_driver_id'].iloc[-1]\n",
    "\n",
    "        #first instance of driver id to track switches\n",
    "        first_instance = group[group['hash_driver_id'] == driver_id].iloc[0]\n",
    "\n",
    "        time_id = first_instance['location_read_at']\n",
    "\n",
    "        #filter out pings before switch\n",
    "        valid_group = group[group['location_read_at'] >= time_id]\n",
    "\n",
    "        lngs = valid_group['longitude'].tolist()\n",
    "        lats = valid_group['latitude'].tolist()\n",
    "\n",
    "        #dist gaps\n",
    "        dist_gaps = [0]\n",
    "        time_gaps = [0]\n",
    "        prev_lat = lats[0]\n",
    "        prev_lng = lngs[0]\n",
    "        prev_time = time_id\n",
    "        cum_dist = 0\n",
    "\n",
    "        #total distance\n",
    "        for lat, lng in zip(lats[1:], lngs[1:]):\n",
    "            dist = calculate_distance(prev_lat, prev_lng, lat, lng)\n",
    "            cum_dist += dist\n",
    "            dist_gaps.append(cum_dist)\n",
    "            prev_lat = lat\n",
    "            prev_lng = lng\n",
    "\n",
    "        total_dist = cum_dist\n",
    "\n",
    "        #time gaps\n",
    "        time_gaps = [(t - time_id).total_seconds() for t in valid_group['location_read_at']]\n",
    "\n",
    "        #total time\n",
    "        total_time = calculate_time_difference(valid_group['location_read_at'].iloc[0], valid_group['location_read_at'].iloc[-1])\n",
    "\n",
    "        trip_data.append([booking_id, driver_id, time_id, lngs, lats, total_dist, total_time, time_gaps, dist_gaps])\n",
    "\n",
    "    output_df = pd.DataFrame(trip_data, columns=['booking_id', 'driver_id', 'time_id', 'lngs', 'lats', 'dist', 'time', 'time_gap', 'dist_gap'])\n",
    "\n",
    "    # Convert DataFrame to list of dicts for JSON processing\n",
    "    trip_data_dicts = output_df.to_dict('records')\n",
    "\n",
    "    # Create JSON objects\n",
    "    json_data = []\n",
    "    date_str = extract_date_from_filename(input_file)\n",
    "\n",
    "    for row in trip_data_dicts:\n",
    "        # Day of the week\n",
    "        week_id, name, date_id = day_of_week(date_str)\n",
    "        # Time ID is minute of day\n",
    "        time_id = time_id_from_timestamp(str(row['time_id']))\n",
    "\n",
    "        new_dict = {\n",
    "            'trip_id': row['booking_id'],\n",
    "            'time_gap': row['time_gap'],\n",
    "            'dist': float(row['dist']),\n",
    "            'lats': row['lats'],\n",
    "            'driverID': row['driver_id'],\n",
    "            'weekID': week_id,\n",
    "            'timeID': time_id,\n",
    "            'dateID': date_id,\n",
    "            'time': float(row['time']),\n",
    "            'lngs': row['lngs'],\n",
    "            'dist_gap': row['dist_gap']\n",
    "        }\n",
    "        json_str = json.dumps(new_dict, separators=(',', ':'))  # Convert to JSON string\n",
    "        json_data.append(json_str)\n",
    "\n",
    "    # Write JSON data to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as output_file:\n",
    "        for entry in json_data:\n",
    "            output_file.write(entry + '\\n')\n",
    "            output_file.flush()\n",
    "\n",
    "# Process each input file\n",
    "for input_file in input_files:\n",
    "    date_str = input_file.split('_')[-1].split('.')[0]  # Extract date from file name\n",
    "    output_file = os.path.join(output_dir, f'traj_fix_dist_{date_str}.json')\n",
    "    process_file(os.path.join(input_dir, input_file), output_file)\n",
    "    print(f\"Data has been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract clean trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and cleaned data for date: 2024-01-22\n",
      "Processed and cleaned data for date: 2024-01-11\n",
      "Processed and cleaned data for date: 2024-01-23\n",
      "Processed and cleaned data for date: 2024-01-01\n",
      "Processed and cleaned data for date: 2024-01-21\n",
      "Processed and cleaned data for date: 2024-01-06\n",
      "Processed and cleaned data for date: 2024-01-12\n",
      "Processed and cleaned data for date: 2024-01-05\n",
      "Processed and cleaned data for date: 2024-01-17\n",
      "Processed and cleaned data for date: 2024-01-24\n",
      "Processed and cleaned data for date: 2024-01-09\n",
      "Processed and cleaned data for date: 2024-01-15\n",
      "Processed and cleaned data for date: 2024-01-13\n",
      "Processed and cleaned data for date: 2024-01-04\n",
      "Processed and cleaned data for date: 2024-01-14\n",
      "Processed and cleaned data for date: 2024-01-16\n",
      "Processed and cleaned data for date: 2024-01-03\n",
      "Processed and cleaned data for date: 2024-01-18\n",
      "Processed and cleaned data for date: 2024-01-08\n",
      "Processed and cleaned data for date: 2024-01-19\n",
      "Processed and cleaned data for date: 2024-01-02\n",
      "Processed and cleaned data for date: 2024-01-20\n",
      "Processed and cleaned data for date: 2024-01-10\n",
      "Processed and cleaned data for date: 2024-01-07\n",
      "Matching process completed. Check the 'trial week/clean_data/' directory for results.\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'clean_data/json_traj/'\n",
    "output_dir = 'clean_data/'\n",
    "clean_file_template = \"clean_{date}.json\"\n",
    "\n",
    "# Load pooling data\n",
    "poolin_dir = 'careems_data/'\n",
    "dp = []\n",
    "\n",
    "# Loop through all pooling files \n",
    "for file_name in os.listdir(poolin_dir):\n",
    "    if file_name.startswith('anon_pooling') and file_name.endswith('.csv'):  # Check file name and extension\n",
    "        file_path = os.path.join(poolin_dir, file_name)\n",
    "        df = pd.read_csv(file_path)  \n",
    "        dp.append(df)  \n",
    "pooling_data = pd.concat(dp, ignore_index=True)\n",
    "\n",
    "\n",
    "for j in os.listdir(input_dir):\n",
    "    date_str = j.split('_')[1].split('.')[0]  # Extract date from file name\n",
    "    filtered_pooling_data = pooling_data[pooling_data['day'] == date_str]\n",
    "    \n",
    "    if filtered_pooling_data.empty:\n",
    "        print(f\"No pooling data for date: {date_str}\")\n",
    "        continue\n",
    "\n",
    "    input_file_path = os.path.join(input_dir, j)\n",
    "    clean_file_path = os.path.join(output_dir, clean_file_template.format(date=date_str))\n",
    "\n",
    "    with open(input_file_path, \"r\") as json_traj_file:\n",
    "        new_data = [json.loads(line) for line in json_traj_file]  # Read each line as a JSON object\n",
    "\n",
    "    with open(clean_file_path, 'w', encoding='utf-8') as clean_file:\n",
    "        for entry in new_data:\n",
    "            entry_time = entry[\"time\"]\n",
    "            entry_trip_id = entry[\"trip_id\"]\n",
    "            entry_driver_id = entry[\"driverID\"]\n",
    "\n",
    "            for _, row in filtered_pooling_data.iterrows():\n",
    "                pool_time = float(row[\"captain_engagement_time\"] * 60)\n",
    "                pool_trip_id = row[\"booking_id\"]\n",
    "                pool_driver_id = row[\"captain_id\"]\n",
    "                time_diff = pool_time - entry_time\n",
    "\n",
    "                # Adding time difference to entry\n",
    "                entry_with_time_diff = entry.copy()\n",
    "                entry_with_time_diff[\"time_diff\"] = time_diff\n",
    "\n",
    "                # Writing good trips to clean file (same trip and driver id and <= 5 sec time diff)\n",
    "                if entry_trip_id == pool_trip_id and entry_driver_id == pool_driver_id and abs(time_diff) <= 5:\n",
    "                    json.dump(entry_with_time_diff, clean_file)\n",
    "                    clean_file.write(\"\\n\")\n",
    "                    break\n",
    "\n",
    "    print(f\"Processed and cleaned data for date: {date_str}\")\n",
    "\n",
    "print(\"Matching process completed. Check the 'trial week/clean_data/' directory for results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Splitting Trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "from shapely.ops import unary_union\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merchant data\n",
    "merchant_file_path = 'careems_data/order_merchant_id_anon.parquet'\n",
    "merchants_df = pd.read_parquet(merchant_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pooling data\n",
    "directory = 'careems_data/'\n",
    "dp = []\n",
    "\n",
    "# Loop through all pooling files \n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.startswith('anon_pooling') and file_name.endswith('.csv'):  # Check file name and extension\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        df = pd.read_csv(file_path)  \n",
    "        dp.append(df)  \n",
    "\n",
    "\n",
    "df_pooling = pd.concat(dp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean data and drop time_diff column\n",
    "directory = 'clean_data/'\n",
    "dp = []\n",
    "\n",
    "# Loop through all clean data files \n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.startswith('clean_2024-') and file_name.endswith('.json'):  # Check file name and extension\n",
    "        month = int(file_name.split('-')[1])  # Extract the month and convert to int\n",
    "        file_path = os.path.join(directory, file_name)  \n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:  \n",
    "                entry = json.loads(line)  \n",
    "                entry['month'] = month  # Add month to the data\n",
    "                dp.append(entry) \n",
    "\n",
    "\n",
    "df = pd.DataFrame(dp)\n",
    "\n",
    "if 'time_diff' in df.columns:\n",
    "    df.drop('time_diff', axis=1, inplace=True)\n",
    "\n",
    "df = df.rename(columns={'time': 'trip_time'})\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left join merchants on the pings df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Merge merchants_df with df_pooling on 'order_id'\n",
    "merged_pooling = df_pooling.merge(merchants_df[['order_id', 'merchant_id']], on='order_id', how='left')\n",
    "\n",
    "# Merge with df on 'trip_id' \n",
    "df = df.merge(merged_pooling[['booking_id', 'merchant_id']], left_on='trip_id', right_on='booking_id', how='left')\n",
    "\n",
    "df = df.drop(columns=['booking_id'])\n",
    "\n",
    "# Check how many 'merchant_id' values are null \n",
    "print(df['merchant_id'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Stationarity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all stationary intervals over each trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_stationary_intervals(df):\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = np.array(row['lats'])\n",
    "        lngs = np.array(row['lngs'])\n",
    "        time_gap = np.array(row['time_gap'])\n",
    "        \n",
    "        # Compute differences between consecutive points directly from lat/lng arrays\n",
    "        lat_diff = np.abs(np.diff(lats))\n",
    "        lng_diff = np.abs(np.diff(lngs))\n",
    "\n",
    "        # Identify stationary intervals\n",
    "        stationary_indices = np.where((lat_diff == 0) & (lng_diff == 0))[0]\n",
    "        \n",
    "        # Group consecutive stationary indices into intervals\n",
    "        if len(stationary_indices) > 0:\n",
    "            start_idx = stationary_indices[0]\n",
    "            for i in range(1, len(stationary_indices)):\n",
    "                # If the current index is not consecutive, close the interval\n",
    "                if stationary_indices[i] != stationary_indices[i - 1] + 1:\n",
    "                    end_idx = stationary_indices[i - 1]\n",
    "                    # Save the interval only if it has more than one index\n",
    "                    if end_idx > start_idx:\n",
    "                        interval = {\n",
    "                            \"trip_id\": trip_id,\n",
    "                            \"start_idx\": int(start_idx),\n",
    "                            \"end_idx\": int(end_idx)+1,\n",
    "                            \"start_lat\": float(lats[start_idx]),\n",
    "                            \"start_lng\": float(lngs[start_idx]),\n",
    "                            \"end_lat\": float(lats[end_idx+1]),\n",
    "                            \"end_lng\": float(lngs[end_idx+1]),\n",
    "                            \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                        }\n",
    "                        results.append(interval)\n",
    "                    # Start a new interval\n",
    "                    start_idx = stationary_indices[i]\n",
    "            # Add the last interval if it has more than one index\n",
    "            end_idx = stationary_indices[-1]\n",
    "            if end_idx > start_idx:\n",
    "                interval = {\n",
    "                    \"trip_id\": trip_id,\n",
    "                    \"start_idx\": int(start_idx),\n",
    "                    \"end_idx\": int(end_idx)+1,\n",
    "                    \"start_lat\": float(lats[start_idx]),\n",
    "                    \"start_lng\": float(lngs[start_idx]),\n",
    "                    \"end_lat\": float(lats[end_idx+1]),\n",
    "                    \"end_lng\": float(lngs[end_idx+1]),\n",
    "                    \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                }\n",
    "                results.append(interval)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "stationary_df_pre_truncation = detect_stationary_intervals(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate trips where a stationary interval exists at the end of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def truncate_trips_if_stationary(df, stationary_df):\n",
    "    truncated_trips = []\n",
    "\n",
    "    # Iterate over each trip in the df\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = row['lats']\n",
    "        lngs = row['lngs']\n",
    "        time_gap = row['time_gap']\n",
    "        dist_gap = row['dist_gap']\n",
    "        dist = row['dist']\n",
    "        trip_time = row['trip_time']\n",
    "        month = row['month']\n",
    "\n",
    "        # Get the last coordinates of the trip\n",
    "        last_lat, last_lng = lats[-1], lngs[-1]\n",
    "\n",
    "        # Find the corresponding stationary intervals for this trip\n",
    "        intervals = stationary_df[stationary_df['trip_id'] == trip_id]\n",
    "\n",
    "        if not intervals.empty:\n",
    "            # Get the last stationary interval for this trip\n",
    "            last_interval = intervals.iloc[-1]\n",
    "\n",
    "            interval_lat, interval_lng = last_interval['end_lat'], last_interval['end_lng']\n",
    "            time_elapsed = last_interval['time_elapsed']\n",
    "\n",
    "            # Check if the last stationary interval's coordinates match the last trip coordinates\n",
    "            if (last_lat == interval_lat and last_lng == interval_lng) and time_elapsed > 10:\n",
    "                # Keep only the first part of the stationary segment\n",
    "                truncated_trip = {\n",
    "                    'trip_id': trip_id,\n",
    "                    'lats': lats[:last_interval['start_idx'] + 1],\n",
    "                    'lngs': lngs[:last_interval['start_idx'] + 1],\n",
    "                    'time_gap': time_gap[:last_interval['start_idx'] + 1],\n",
    "                    'dist_gap': dist_gap[:last_interval['start_idx'] + 1],\n",
    "                    'dist': dist,  # Keep the original distance\n",
    "                    'trip_time': time_gap[last_interval['start_idx']],  # Adjusted trip time\n",
    "                    'driverID': row['driverID'],\n",
    "                    'weekID': row['weekID'],\n",
    "                    'timeID': row['timeID'],\n",
    "                    'dateID': row['dateID'],\n",
    "                    'merchant_id': row['merchant_id'],\n",
    "                    'month': month\n",
    "                }\n",
    "                truncated_trips.append(truncated_trip)\n",
    "            else:\n",
    "                # If no truncation is needed, keep the original trip\n",
    "                truncated_trips.append(row.to_dict())\n",
    "        else:\n",
    "            # If no stationary intervals exist, keep the original trip\n",
    "            truncated_trips.append(row.to_dict())\n",
    "\n",
    "    # Convert the list of truncated trips back into a df\n",
    "    truncated_df = pd.DataFrame(truncated_trips)\n",
    "\n",
    "    # Ensure all columns match the original schema\n",
    "    for column in df.columns:\n",
    "        if column not in truncated_df.columns:\n",
    "            truncated_df[column] = None  # Add missing columns with default None values\n",
    "\n",
    "    # Reorder columns to match the original DataFrame\n",
    "    truncated_df = truncated_df[df.columns]\n",
    "\n",
    "    return truncated_df\n",
    "\n",
    "truncated_df = truncate_trips_if_stationary(df, stationary_df_pre_truncation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find new stationary intervals post truncation and longest interval for each trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_stationary_intervals(df):\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = np.array(row['lats'])\n",
    "        lngs = np.array(row['lngs'])\n",
    "        time_gap = np.array(row['time_gap'])\n",
    "        \n",
    "        # Compute differences between consecutive points directly from lat/lng arrays\n",
    "        lat_diff = np.abs(np.diff(lats))\n",
    "        lng_diff = np.abs(np.diff(lngs))\n",
    "\n",
    "        # Identify stationary intervals \n",
    "        stationary_indices = np.where((lat_diff == 0) & (lng_diff == 0))[0]\n",
    "        \n",
    "        # Group consecutive stationary indices into intervals\n",
    "        if len(stationary_indices) > 0:\n",
    "            start_idx = stationary_indices[0]\n",
    "            for i in range(1, len(stationary_indices)):\n",
    "                # If the current index is not consecutive, close the interval\n",
    "                if stationary_indices[i] != stationary_indices[i - 1] + 1:\n",
    "                    end_idx = stationary_indices[i - 1]\n",
    "                    # Save the interval only if it has more than one index\n",
    "                    if end_idx > start_idx:\n",
    "                        interval = {\n",
    "                            \"trip_id\": trip_id,\n",
    "                            \"start_idx\": int(start_idx),\n",
    "                            \"end_idx\": int(end_idx)+1,\n",
    "                            \"start_lat\": float(lats[start_idx]),\n",
    "                            \"start_lng\": float(lngs[start_idx]),\n",
    "                            \"end_lat\": float(lats[end_idx+1]),\n",
    "                            \"end_lng\": float(lngs[end_idx+1]),\n",
    "                            \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                        }\n",
    "                        results.append(interval)\n",
    "                    # Start a new interval\n",
    "                    start_idx = stationary_indices[i]\n",
    "            # Add the last interval if it has more than one index\n",
    "            end_idx = stationary_indices[-1]\n",
    "            if end_idx > start_idx:\n",
    "                interval = {\n",
    "                    \"trip_id\": trip_id,\n",
    "                    \"start_idx\": int(start_idx),\n",
    "                    \"end_idx\": int(end_idx)+1,\n",
    "                    \"start_lat\": float(lats[start_idx]),\n",
    "                    \"start_lng\": float(lngs[start_idx]),\n",
    "                    \"end_lat\": float(lats[end_idx+1]),\n",
    "                    \"end_lng\": float(lngs[end_idx+1]),\n",
    "                    \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                }\n",
    "                results.append(interval)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "stationary_df = detect_stationary_intervals(truncated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=truncated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_longest_stationary_interval(stationary_df):\n",
    "    # Find the longest stationary interval for each trip_id based on time_elapsed\n",
    "    longest_intervals = stationary_df.loc[stationary_df.groupby('trip_id')['time_elapsed'].idxmax()]\n",
    "\n",
    "    longest_intervals.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return longest_intervals\n",
    "\n",
    "longest_intervals_df = extract_longest_stationary_interval(stationary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combined df with pooling coordinates and longest stationary interval coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined(longest_intervals_df, pooling_df):\n",
    "    \n",
    "    # Rename 'booking_id' to 'trip_id' in pooling_df for consistency\n",
    "    pooling_df = pooling_df.rename(columns={'booking_id': 'trip_id'})\n",
    "    \n",
    "    combined_df = (\n",
    "        longest_intervals_df[['trip_id', 'start_lat', 'start_lng']]\n",
    "        .merge(pooling_df[['trip_id', 'pickup_latitude', 'pickup_longitude']], on='trip_id', how='inner')\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity in the combined data\n",
    "    combined_df = combined_df.rename(columns={\n",
    "        'start_lat': 'stationary_df_lat',\n",
    "        'start_lng': 'stationary_df_lng',\n",
    "        'pickup_latitude': 'pooling_df_lat',\n",
    "        'pickup_longitude': 'pooling_df_lng'\n",
    "    })   \n",
    "    return combined_df\n",
    "\n",
    "combined_df = create_combined(longest_intervals_df, df_pooling)\n",
    "\n",
    "combined_df = combined_df.merge(df[['trip_id', 'merchant_id']], on='trip_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update incorrect pooling coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inconsistent groups: 1775\n"
     ]
    }
   ],
   "source": [
    "def check_inconsistent_groups_with_tolerance_fix(combined_df):\n",
    "    \n",
    "    # Lists to store inconsistent groups and detailed non-unique counts\n",
    "    inconsistent_groups = []\n",
    "    nonunique_info = []\n",
    "\n",
    "    for merchant_id, group in combined_df.groupby('merchant_id'):\n",
    "        # Extract latitude and longitude values for the group\n",
    "        lat_values = group['pooling_df_lat'].values\n",
    "        lng_values = group['pooling_df_lng'].values\n",
    "\n",
    "        # Count occurrences of each latitude and longitude\n",
    "        lat_counts = pd.Series(lat_values).value_counts()\n",
    "        lng_counts = pd.Series(lng_values).value_counts()\n",
    "\n",
    "        # Identify the majority (most common) latitude and longitude\n",
    "        majority_lat = lat_counts.idxmax()\n",
    "        majority_lng = lng_counts.idxmax()\n",
    "\n",
    "        # Filter out the non-majority coordinates\n",
    "        non_majority_lats = lat_counts[lat_counts.index != majority_lat]\n",
    "        non_majority_lngs = lng_counts[lng_counts.index != majority_lng]\n",
    "\n",
    "        # If there are non-majority coordinates, store the information\n",
    "        if not non_majority_lats.empty or not non_majority_lngs.empty:\n",
    "            inconsistent_groups.append(group)\n",
    "\n",
    "            # Store the non-unique information for this merchant_id\n",
    "            nonunique_info.append({\n",
    "                'merchant_id': merchant_id,\n",
    "                'non_majority_lats': non_majority_lats.to_dict(),  # Non-majority latitudes and their counts\n",
    "                'non_majority_lngs': non_majority_lngs.to_dict(),  # Non-majority longitudes and their counts\n",
    "                'group_size': len(group)\n",
    "            })\n",
    "\n",
    "    # Combine all inconsistent groups into a single DataFrame\n",
    "    if inconsistent_groups:\n",
    "        print(f\"Number of inconsistent groups: {len(inconsistent_groups)}\")\n",
    "        non_majority_df = pd.DataFrame(nonunique_info)\n",
    "    else:\n",
    "        print(\"No inconsistencies found.\")\n",
    "\n",
    "    return non_majority_df\n",
    "\n",
    "non_majority_df = check_inconsistent_groups_with_tolerance_fix(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change incorrect pooling coordinates in combined_df to the majority coordinates for each merchant\n",
    "def correct_pooling_coordinates_with_majority(non_majority_df, combined_df):\n",
    "\n",
    "    # Iterate over each merchant in the non-majority df\n",
    "    for _, row in non_majority_df.iterrows():\n",
    "        merchant_id = row['merchant_id']\n",
    "\n",
    "        # Get the majority latitude and longitude for this merchant from combined_df\n",
    "        majority_lat = combined_df[combined_df['merchant_id'] == merchant_id]['pooling_df_lat'].mode()[0]\n",
    "        majority_lng = combined_df[combined_df['merchant_id'] == merchant_id]['pooling_df_lng'].mode()[0]\n",
    "\n",
    "        # Find the trip_ids with non-majority coordinates\n",
    "        merchant_group = combined_df[combined_df['merchant_id'] == merchant_id]\n",
    "        incorrect_trips = merchant_group[\n",
    "                (merchant_group['pooling_df_lat'] != majority_lat) |\n",
    "                (merchant_group['pooling_df_lng'] != majority_lng)\n",
    "            ]\n",
    "\n",
    "        # Update the coordinates in combined_df for the incorrect trips\n",
    "        for trip_id in incorrect_trips['trip_id']:\n",
    "            combined_df.loc[combined_df['trip_id'] == trip_id, ['pooling_df_lat', 'pooling_df_lng']] = [\n",
    "                majority_lat, majority_lng\n",
    "            ]\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "combined_df = correct_pooling_coordinates_with_majority(non_majority_df, combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract non-matching coordinates and drop them from df and longest stationary df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8074/54936362.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_matching_df['distance_km'] = haversine(\n"
     ]
    }
   ],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "\n",
    "    # Compute differences\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Apply Haversine formula\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "def extract_non_matching_coordinates(combined_df):\n",
    "    # Identify rows where the stationary coordinates do not match pooling coordinates\n",
    "    non_matching_df = combined_df[\n",
    "        (combined_df['stationary_df_lat'] != combined_df['pooling_df_lat']) |\n",
    "        (combined_df['stationary_df_lng'] != combined_df['pooling_df_lng'])\n",
    "    ]\n",
    "\n",
    "    # calculate the distance between the coordinates\n",
    "    non_matching_df['distance_km'] = haversine(\n",
    "        non_matching_df['stationary_df_lat'],\n",
    "        non_matching_df['stationary_df_lng'],\n",
    "        non_matching_df['pooling_df_lat'],\n",
    "        non_matching_df['pooling_df_lng']\n",
    "    )\n",
    "\n",
    "    return non_matching_df\n",
    "\n",
    "non_matching_entries = extract_non_matching_coordinates(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrames saved.\n"
     ]
    }
   ],
   "source": [
    "# Drop non-matching trips from `df` and `longest_intervals_df`\n",
    "def drop_non_matching_trips(df, longest_intervals_df, non_matching_entries):\n",
    "    # Extract the trip IDs from non-matching entries\n",
    "    non_matching_trip_ids = non_matching_entries['trip_id'].unique()\n",
    "\n",
    "    # Drop these trips from both dfs\n",
    "    df_filtered = df[~df['trip_id'].isin(non_matching_trip_ids)]\n",
    "    longest_intervals_filtered = longest_intervals_df[~longest_intervals_df['trip_id'].isin(non_matching_trip_ids)]\n",
    "\n",
    "    return df_filtered, longest_intervals_filtered\n",
    "\n",
    "\n",
    "df, longest_intervals_df = drop_non_matching_trips(df, longest_intervals_df, non_matching_entries)\n",
    "print(\"Filtered DataFrames saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Splitting trips into 3 segments (Driver-to-Merchant, Wait-Time-at-Merchant, Merchant-to-Customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split trips based on stationary intervals matching coordinates from pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No interval found for trip_id: 65d99e964c50a5eed233fe9a7864114c1d91cf6b6ba542bbf4906a998d4df27f\n",
      "No interval found for trip_id: 7d71d55894365ccad46d7e7c533997c58e200cc235919b0c817944af8b5226a1\n",
      "No interval found for trip_id: 218225707aa59531afe509f5ff2b52200dbdd449550265a226f348a814421952\n",
      "No interval found for trip_id: bec1e8f0b569297918a23501ea481f66e0d3417dd07f96b91b7db23d257bd8d1\n",
      "No interval found for trip_id: cc31cd6a0490a3f2ba6811353e2f2339817b2b7a3088e4ad26dae93ad6159f9e\n",
      "No interval found for trip_id: 2d3579c7f52c46c1bb872ff4c4a7ffe5c1c44cebf887bb56081c8f52f2039949\n",
      "No interval found for trip_id: 71e5adb6c386c8735b7d0c84db5bcd80390d36d3e35a459dd544df0405deb66e\n",
      "No interval found for trip_id: 764938195bf3e3ff342db9708afc6a92b72ce7c8b41137a15bca38a910c4722e\n",
      "No interval found for trip_id: 025c623bc8fc62c7b3f4618767d39078a94b96156bf307c010a33ab270988e26\n",
      "No interval found for trip_id: aebd4aab90fe81bc19cb4c0be8a13250b63201a9059205e56a21caa496920125\n",
      "No interval found for trip_id: d415bf3c4be82ea1485134055e7e3a44d190ef6b474bfb95b66c88e5fa5e3904\n",
      "No interval found for trip_id: fd08dcf0151cc467908ce18721f1f1b49a1b0feb747436228e9f4f3b841c5eb0\n",
      "No interval found for trip_id: 3a72aee2078aada914d8fc2c2b453f33eca7cf9a8817e9ce0db4ea7f313bbea7\n",
      "No interval found for trip_id: 630383cb27a79b3e313dc2253a1059d15986fa1002bde3d4375e348337efc7e1\n",
      "No interval found for trip_id: a16cf487526d603fd09be1f80d55def2e31c2d52961d3d97e3bc9b2f01caf891\n",
      "No interval found for trip_id: e23fd958a9e7e616ff6cfe187b377698a86c16f9009b880b47a1178294ca0995\n",
      "No interval found for trip_id: 827187a3c516d81d503444ed78e77828702baa5a444ee87382161e9802c012de\n",
      "No interval found for trip_id: c68816f8bab5b6866a8517a5789f91dc5d128c521f397e78e62904a3e2713a87\n",
      "No interval found for trip_id: f85ce75f18a1294a48112690df8aa176ae271b5d56eaa3eb1271c22c46ff2f3f\n",
      "No interval found for trip_id: 418227eb45540803e2c286ff52393209881b72a1bef0706d63d714ab6343a7e3\n",
      "No interval found for trip_id: 70a5065cd68bc0f3618786af9c8fb84bb730673bb44047bd55d380acb61492ec\n",
      "No interval found for trip_id: 9bbbb7e76ad4395ecbee3cd0f4e5ecc6a308f126c593c71f8d78d792902d6a44\n",
      "No interval found for trip_id: dfc8ab4891bd05baed1cf438065d20b4b6bfc2d7c6a404d282024f1e77cd8130\n",
      "No interval found for trip_id: e66567421fcc7240b12c374ef7668dc6395f927d80c4fcd48022cc9209474bd8\n",
      "No interval found for trip_id: 6e9cbcc17a8a7e1956c7964c8eeb97631f19dff413ed46d1ffaad16088cbc6b5\n",
      "No interval found for trip_id: e11915f1a75885957d9d496ad25fa0defc26acfdb1d960a995120bd85ade510c\n",
      "No interval found for trip_id: f5495060e566ae3a5f77e80a0813efccec09f3811f08059169e667ff0cc80ec8\n",
      "No interval found for trip_id: dd1081677860f88a87b94c12f92bf3daff18e5f581b03ded9d154dfcdf0eb4a8\n",
      "No interval found for trip_id: 4607059212a19e58ec158796e8293b0fc5163b26d7cb28e41e9b9cc29c2880aa\n",
      "No interval found for trip_id: 89892dd6159e1eb5d369864f6e0d59740470f589de72263c4e5cd4c032866d54\n",
      "No interval found for trip_id: aed464d1545a6df73a6757363684e37de599b53a7a2ec144f7c73750bcf5a8d4\n",
      "No interval found for trip_id: b2e9cb3ba259f223b4fcbf63c41a0ec6c5a7ceba6001751e36de39f045459f5e\n",
      "No interval found for trip_id: cd7233d7b9a39a9d36763fa297be79f8dc44b5c8accd396ade0362aa667c91ae\n",
      "No interval found for trip_id: d72c65c198afdcec761ff9a290fab75f804bba85ee82edf90dc234dfc6e7f8a8\n",
      "No interval found for trip_id: 06b2b3115d0550da4057579578275662f7059ed10e74bdec50d5c134096d41e4\n",
      "No interval found for trip_id: 0d2604ecd765fd135ab532bbcf3203103f67df178401d6e492a3807783dc7e91\n",
      "No interval found for trip_id: 511617042ed6671e55effc726ebed08b66cfb97eed7b387da076fb4e9d425278\n",
      "No interval found for trip_id: e46cc6c5f3661de787f489b126c91b87a025a50a9a566286c51cc46fe932b956\n",
      "No interval found for trip_id: 06cc1be7f70821862444021ddaafcd6ed1a63747cb299395f492cc46f3f3670c\n",
      "No interval found for trip_id: 76ccee518f9f9085275a98f87c9852007f93e304edcc791a109c8cfb68c4ea26\n",
      "No interval found for trip_id: 99718da7625cb9b246b14f14a33c8cc062d222881ef8720900a9b167ab3f1dbe\n",
      "No interval found for trip_id: e9834e07f9ef6e17922926583ebc73d74c9e2bd4230778d07b655537f22d354c\n",
      "No interval found for trip_id: edc045056a5a848dacfccc872c9865c31fa3263808185c23487f15dc1e2ce339\n",
      "No interval found for trip_id: fe79dfe61dc301924e775ff1706d14e83aa22322466f142065dbc5f951f1b0d3\n",
      "No interval found for trip_id: 2b82ba6cb112690dfe917d9cd37f942ec01da81202e230c19256cb20d63f7892\n",
      "No interval found for trip_id: 7bd485199316d8e9081a801cc7d699622e3fd8547e028f1e10640700a21c75ef\n",
      "No interval found for trip_id: fc5c3baa52d64b8a751d2260940c54b06cc69fda6d9b5f75699316264955aa8a\n",
      "No interval found for trip_id: d7ac9fa915165761ac734835acab4167e4d41d12eae632de9657a92c4033ff8e\n",
      "No interval found for trip_id: 41f0387e91c9b8d7fa9fcc194b2e677a096ad80f03c3ed701be73befc7256258\n",
      "No interval found for trip_id: fe6e216e50ba5e5f78d40e8cc1ec59781bf720f7fe56d04ed3f5c4cc945811cf\n",
      "No interval found for trip_id: 3dda4ee2fe8ba8147983fe9d5cd19953cfdbe0c620bd58eb7dfe26c3ce7697e6\n",
      "No interval found for trip_id: 60c9942b9d2938bf87edf0ff63d0ac3f2a26fbca33393c8e286209cad6ea8368\n",
      "No interval found for trip_id: b7de07d8de07850e4ecd093247e72511afbcd747004ebe010fdec6498c694e24\n",
      "No interval found for trip_id: d06bbef61c2e66f4727d3792d5e31a00f4dac7469d7ab8f4b4b3a2ba2d2edb34\n",
      "No interval found for trip_id: 32c9acf85360fb47de0897edf9ad1ad78aa0104fa0b5a928854bd9dfe5e408c5\n",
      "No interval found for trip_id: 43586f658ec179c39ad9a66c4a0af6d0b0b85ce8005799786155226d19210f3d\n",
      "No interval found for trip_id: fb48e1c8db26a90129f6ee00f79ecab9406278ad58ab4372f9a6cee3dac3f5e6\n",
      "No interval found for trip_id: 2bfac26ef438eaea202017ab4142c3e5d525a0cf50baf749087cbde5d9fb77b3\n",
      "No interval found for trip_id: 606ba08253c8b388b23c9e190f2aba16cc0f81e8784bec5a1bde182f736e09f7\n",
      "No interval found for trip_id: 6e9fd501349b9bd7b7c054ce12993ba2311f7e7b4674fb70140f5ca218e6a4a5\n",
      "No interval found for trip_id: 75ce2472be8f2f9ef668baa331c8dff0f06c013d858f4d1a2ed1a4d37800400f\n",
      "No interval found for trip_id: 8570f4cd7cd995f21294fe3b0566a70f04f3e354702111d537ff872c603cdbc3\n",
      "No interval found for trip_id: 8bffa67adf80f12fff5f6f451c5bbe166b23c5ba3546fb6ba33f147c6673c0dd\n",
      "No interval found for trip_id: ed9ca8f68a6b8a9c62ac1c96bece12c4574b42e3351af20935aff1264fd996b9\n",
      "No interval found for trip_id: 07a2f3cdf28096c27c4c2b81bcfafe30a96095097f6e96e79878f714a9ec648c\n",
      "No interval found for trip_id: 6299a27e7d370d6904a05073fdd554e0efbf09f0d9458d7e0f677e75644c2e34\n",
      "No interval found for trip_id: 53b6dcc15dc4dfcb6c7fdbc9bf34f3413dbf6453e046c912e6645fb3b1ca5147\n",
      "No interval found for trip_id: cafc982eadc157a3e71b809ee76af9c7dfc30c150a1199402fa5beb0faa8d137\n",
      "No interval found for trip_id: fbb3bca8c359fb81d97d0a50973f252c6d52eb1e38236a574a8e9c29db0c41c8\n",
      "No interval found for trip_id: 55d06db7ce251221a3ea491b0c0052dadebd69ba3006b493d9ff618785106996\n",
      "No interval found for trip_id: ac20a006ff77d0de3ff117416079e9f4964ed48f9558675d9871fafd620bcc36\n",
      "No interval found for trip_id: 7fb0deb52d20be82647eee6094e17703a2e9328754840cb3f7f71502949e5ee9\n",
      "No interval found for trip_id: adcdb90a3bd604f316e19b5d611c18d77cd4143b480a8983aee109121f69d2c4\n",
      "No interval found for trip_id: b7b0865eadddb0bfb378189504dcafa1e4e8b6abe4573cd058a07c86f5240edc\n",
      "No interval found for trip_id: c87ffc20c6cf05ad983eb81178f75c36566491861b7a11d646299c1b91ed7161\n",
      "No interval found for trip_id: 58cb38950b89b4aad6dd4a32ee69d197952a65ba54de87e0e7a71ae22c2ea6ce\n",
      "No interval found for trip_id: 35eb23a5ef4a2ea073a83d60f6c5bfd77111f100688cbf2a2ed9dddc9a90aeba\n",
      "No interval found for trip_id: 553a5a3eac65d331a068a8a60c29ce6c3ce4bc4cff4c63542f2d45cdc62bd55f\n",
      "No interval found for trip_id: 64e730589bbba0cb6c3f5b326825303c12de74cce5039304b8d75c4130834950\n",
      "No interval found for trip_id: 95211438a9e97d1e5ca9185f0a6b2c814e399dc323853b0fb85d0d6918d12ca6\n",
      "No interval found for trip_id: 5dc0e366867ddce4caf3a1861cd46b216165cbb1c513cd796e53942b12c9eb1a\n",
      "No interval found for trip_id: b61a3a9ada58f81859560718f1697a84aa9543ac6be94e048b54178ea29dac66\n",
      "No interval found for trip_id: bad44c8533c0435d81119b807997f4836a6648da594f5d4da2ece47bc1bed460\n",
      "No interval found for trip_id: e0e7a3c5ceb935f13325f33aada255dc33fb8e7d343dcd21607e2f8606378005\n",
      "No interval found for trip_id: f9a051e1debe7d4dda61a109a99262a6ae77d6cc261a822badd05242f94bc307\n",
      "No interval found for trip_id: 7217f170967190eb05b1aeda9332849f44cb7ad5c86f85ee4e1ce15af8bb817b\n",
      "No interval found for trip_id: 90b4287b2dcbf37bf60882c25436f664aa8472bd1a4139d5e71ba086eeefdbd0\n",
      "No interval found for trip_id: afa5b6233c3aea45929e82e4bd4aadbf38d652c4305fde7003245137cdec5647\n",
      "No interval found for trip_id: dc743cbc139745fe6c8e9b7e4011651715f559410cce5dbc53224869416f51a2\n",
      "No interval found for trip_id: e3a5004b32365239d3d92267daf9aea266479b762c16e54d37e6a28710b34d3a\n",
      "No interval found for trip_id: 02af99d578f584b75570238faa5023b74785f9d0d83ed288515348528f23037c\n",
      "No interval found for trip_id: 6dd8b861cadee34fc0b0b59dc95f9ef99fcaadfff12091738dfc92ae682fe1e9\n",
      "No interval found for trip_id: 25049734158c2944703546bdc2cbe5740170230779e858db275cb6512aaa0047\n",
      "No interval found for trip_id: 4ad4bdf931291e3f2176bef563091893e0bde118f28a2ef5b172ece38949ed38\n",
      "No interval found for trip_id: 5057cb61e831386cd034d24462a4f6ecebecb3606bc2d39d4c991fd04b56e413\n",
      "No interval found for trip_id: 620950188eebce63b6bf5be22cb280dbb8d75b5cef8904b6de30bb892d2a1c3a\n",
      "No interval found for trip_id: 64186edda7362e34c7598fe8af53c01cde6071b3d932380eaa30299419712632\n",
      "No interval found for trip_id: 0f418d32d9cfc01a697575eb12d466547f5a63cf9ea9d1cc505e5658b0ac15d9\n",
      "No interval found for trip_id: 2d946fb190d92b153e0847d7648e6b452180348a524255ea291e14b53779248b\n",
      "No interval found for trip_id: 3205731c06fc3fbafc924bb60a479448de1832db8fd4895343a1dd71d09251dc\n",
      "No interval found for trip_id: 67e0fd381ee29fb6775a88babca143908e47dd0a45765021e0da2f32e875f7b1\n",
      "No interval found for trip_id: 734549b95d4f1b3250f2638665b882b4c0a266e4c64bd9d1ef5680e7c280b55f\n",
      "No interval found for trip_id: 9ff835866a912cfe82fbda66c603a91e16922e948dc226ed0c112a4d26d326f8\n",
      "No interval found for trip_id: 5e0e6689c29c6a7c27e8e9bf22454b8ae85a9db676b7ca9fb6f30772def5cad2\n",
      "No interval found for trip_id: c5aa91aa8838d9883a72d81058ff5d5054f3f3747a163242de1322e4d1cc3c6b\n",
      "No interval found for trip_id: fa9a26819ed9af5b05ab85ca6e5800402b4c5666bd30d4093c5511bbed84a4b4\n",
      "No interval found for trip_id: c0c86a7bc163f9a9d30a900daaa6ae4cad12e764c8d3c83cce037af701356467\n",
      "No interval found for trip_id: 1815df14278b78aa665d14ba4d5d64b1c1db869433c838d60d7b2ad89b072687\n",
      "No interval found for trip_id: 40fa6c7155af854a0cca95dd7d5545041815ea1d4cddeeec57fb6b2c66213f1b\n",
      "No interval found for trip_id: d26caa9e14e032c91dc6a69c4cc4f64b11656cd36706596bc7cc390d5cf8b956\n",
      "No interval found for trip_id: 8b22f1e83797c76b5272b9ab75e52c66aff489e3da7593f643c2f5711e44411e\n",
      "No interval found for trip_id: c8de4fe8adaf92f61f94701c1a598bed7fc8057772e566e74e3e69bc64709ac2\n",
      "No interval found for trip_id: 30ef7edb9746d303f56c4e82c8c83f0a2e7989b5f2e09006dd8bf835aad320b1\n",
      "No interval found for trip_id: 3fe5f184bdc721f8c9d72b825717169a7194c8d77d13b9107c6210deda51a2ec\n",
      "No interval found for trip_id: 78a1510497540eab71f626e002d248a5fe135d1e752d4b50f046d3c7aee49070\n",
      "No interval found for trip_id: 9b3aad6386ddbfc4b739f0de172c3838ec9be27b4be2e410dee1bc497a6c1cb2\n",
      "No interval found for trip_id: 9e2183bf7e903707a43d5c413fb2bbfa4e33a93d1a95161d4b13a23e9a1dfea2\n",
      "No interval found for trip_id: d0675e7bb18d35214d2e6c5cbea1a5c31e4528d59dfe764d78fa2d7621893ed5\n",
      "No interval found for trip_id: 503556980147016ebf664b1440e460f5747e860585cca8ed6981af2da79b176c\n",
      "No interval found for trip_id: 27d8615cd80ab509a2e3a230f8d597f6008b6c197074c7c75db40ab69700b6fd\n",
      "No interval found for trip_id: 1921d62f25c5f5499b40c21a7477300ad0232c7e1fc57c71e51e96e0bc5f6a93\n",
      "No interval found for trip_id: fef8302b168d7851808a61844c8284bab7240c54bd9f96219e9cb6ecaa67c8a4\n",
      "No interval found for trip_id: 767dacdad0c8ce4c8a8105ccd693a19da005955cd83a81457aebe80d936c722e\n",
      "No interval found for trip_id: 27c2f2d76c2b4a7ce6b32b66580dac63cf62686b8b2bf8632cc96a12647ab5b7\n",
      "No interval found for trip_id: 456bac5b388bff766811e0d3d1aa747e4a2f21114ef80835216fab56dc162b52\n",
      "No interval found for trip_id: 9b9c15b7797e3dc6b6602642fbe8158819f0860c87ce12097d5de6c1dc1ca7ff\n",
      "No interval found for trip_id: 76764c938e0eb8fba9e7a634fed879f73545151d8a548698aa692be78c1c6aff\n",
      "No interval found for trip_id: fef50e2b6e2703c7f7672af56bfb49df3a0b23220eeb03acd401fa94eb787f69\n",
      "No interval found for trip_id: 08d42691a20f566e666116e678f3c56651624a768b7a43092a5245acbbc71ace\n",
      "No interval found for trip_id: 4a364cf873fe027a333ee1ad5be259751515200bca58d5352103e0c7b3306aec\n",
      "No interval found for trip_id: 756619c50b17ee83526dda6c0ee4ead83cf50cdb8b76718630c9f10aa6e09119\n",
      "No interval found for trip_id: a8178a79f9a2b059f4e951777992d4e6c36e3782a5767e4010f0831a46e36d6a\n",
      "No interval found for trip_id: e0933d83f8c0492912d0ba4418433f3114df0d6ae9c320ae2d62cd2099262b8f\n",
      "No interval found for trip_id: 7b1c447337f1f34968b066298856dad95dadccb41f3fb1361a9193b991c380b5\n",
      "No interval found for trip_id: d48d6dc7f46b4da0a18a05da4482d99bf9eebdc15f054df825ae6cf64a10e01a\n",
      "No interval found for trip_id: d643000d00c596bd3f5cccdf6a628983b954b29f8ca2ce7eb16078b9adc80913\n",
      "No interval found for trip_id: f0a93b637e4370de49027ce6cccdb9d2718991f73f125c4451d8573a263beca0\n",
      "No interval found for trip_id: 03884196cc278ed0084c9ca30734e39819439cb61666dbac89f1ea4477f236f8\n",
      "No interval found for trip_id: 0c16c42d0991c1b8efec94a1a8fa22f0a560240c6756c9aad1b2f52d0410c3f0\n",
      "No interval found for trip_id: 30e3bef99a75f695ca69b36b4f5490883d85dac4a52ac3de1e009b7b4bb290ea\n",
      "No interval found for trip_id: 3fa9adf99bc015e36e4559b2ce84d63ce9ea190ba47852f5446faa777e4a9449\n",
      "No interval found for trip_id: 4f4b416d00065cc662d4764122c3c4edcaa872762e4a70f53ff820e23dfacf71\n",
      "No interval found for trip_id: 5a420885dd1a216b51df7a7895da2e38112e88f2e74b1f56a6ab9bbb41f04d8c\n",
      "No interval found for trip_id: b67379fbc118a3a308f73f462c9c5fdd5e74f02ae0f239257b0c598461bb318c\n",
      "No interval found for trip_id: b9b997916a7e07971cdc3336febb450089e027f34a6d6d878f3be7d6af207ff8\n",
      "No interval found for trip_id: 83c1ec481364ad721ed0763b9f92a2b7385e5d6cc2313ea0cfb3d6eebe9d6546\n",
      "No interval found for trip_id: c04802e9cf60526553d9cbc157dac795599bd8c6e8e6734ffb0c17208617bd95\n",
      "No interval found for trip_id: f9b8c8b97c80b17299534835f22dc26fe4fad3d13a0aed09bdffee32680ae605\n",
      "No interval found for trip_id: 01b8cacb8dbfe3be3c9d036a57f64e6d43b0641fe9abb10b0ff1dc2658ccd5e8\n",
      "No interval found for trip_id: 90ea2065faeb0379972b8150ffa60143a4a0d4e02be5ec73c5e3fd1f28620551\n",
      "No interval found for trip_id: 1b9bc4d236c3260b17fdfbbac40f1f25e9a92ff784ddedf8a019f464b3ae4044\n",
      "No interval found for trip_id: 4b4867c53e88d6453561bb9980979aa5960181a3458057f0d245de6f25b34c1b\n",
      "No interval found for trip_id: b7fd951b19dbe3863619e367b00a6ce17a023105c08d88b1048173b57b572602\n",
      "No interval found for trip_id: 839ab5351052d2dc1a0f096f348c8c0417d62f67f983559c6ea87a61a527d2e6\n",
      "No interval found for trip_id: 99fedc2950cb8ac13f64c946cb4393b05d3bb13af4ab295a8484769c1a9bac4b\n",
      "No interval found for trip_id: 07ae20467370601c988fe25bf2c1377de06abb3d3e1d84f0f2e75fe2efea61f6\n",
      "No interval found for trip_id: 1c39af52fa8812bf3ca221c5b5cbb161e8065f09dc2ec50ef32a373902d2b5f7\n",
      "No interval found for trip_id: 30d2d6f9880131d9cf5664ec556ef379bbe4cabb47d0b8731353cfec7af584e3\n",
      "No interval found for trip_id: 3db0f14055f96b1f3c832995f6d8a39b13cd6a00efceb8d4433fd811fa7ef49d\n",
      "No interval found for trip_id: 59a0d68f700f7618d2f92b02191ee37124446c5b9cbb7dc67c1640a45f53a4e9\n",
      "No interval found for trip_id: 59b705807bded1b9c953fedaf893a4a4fde246df3fa6e00eb8f5b21b40dd0e56\n",
      "No interval found for trip_id: 679070aa851f36ed111e2b826b05ebd21eb8d0de7fdae69df704abbfccea3b87\n",
      "No interval found for trip_id: 82461d84e8a3a72f263110ef10188d401dffe7c356417e892f4686a5887851e5\n",
      "No interval found for trip_id: 8c87ed1c064db83882cbe18368cd5462a06b079694bec7130b6a1203d0c971de\n",
      "No interval found for trip_id: b73dfbfb049c8ae39314c3f573de26d1ecf90b3bac8db68abd808e2bcc3819f4\n",
      "No interval found for trip_id: cd5874ba8e62bc395e066656b17b1911cd1f0d34e2e02b3df21244807e868c22\n",
      "No interval found for trip_id: e0aa414697ee7254b5e52f8503eea0d0fc592dae021bb7d806f6ecfdb2746493\n",
      "No interval found for trip_id: e28e74138ae9a4f1446016c634a682957a96f576746c03075652d1320fe2ec6a\n",
      "No interval found for trip_id: f3f070b7206ce7446f58da07acc0fb5edec6144213698ea2941548da6f1e683e\n",
      "No interval found for trip_id: ad197a65e574c7197feec0a1e885278a5c7c6ab47044d7f8a552bc000cb0ff5a\n",
      "No interval found for trip_id: 32e99056f86a3c0c468fb8795fab97e623856779f05559935a0cbb5c1b102cf9\n",
      "No interval found for trip_id: 7104b8755753df4a5b36011b47d61ffb186016510899441c5a3fa0fef6e25853\n",
      "No interval found for trip_id: 7e8ed98832377d3941da14fcfd4a40a578291fbbdf2e8f6e30311c4ede72f170\n",
      "No interval found for trip_id: bb2e088c05076c4eb380d974cb2905965d03a2592322c7277002a02c1bbec774\n",
      "No interval found for trip_id: ff1be17af8b831ff4bf3cc29d457ac0a3ba4bd16311a5121c9e974b8643169c3\n",
      "No interval found for trip_id: 01ff05c850fb030d8174a48d70a7327747dd0177030bc5cad10f5c9d25b95b33\n",
      "No interval found for trip_id: ec0286dc18471dfaf418922d48c55f7ec72f1e528954b80fc707705137fb5a21\n",
      "No interval found for trip_id: f01f5f9ddc73c41afad46682bccac8a38ccfad813cd19c7bacbde98ae8b96cff\n",
      "No interval found for trip_id: f82b6c7a45db4fd93e76d73881343d2aeeec3fefb876401a4b9380c9ad3cf37d\n",
      "No interval found for trip_id: 643ed78111ac23f84e1a2da7694b1d16ad0642d6b064c2325dee747706da3af7\n",
      "No interval found for trip_id: 7b42a2c96433ba8e3208d0100d34e4c480e48bc36be89122cc31d6fb0fa908fb\n",
      "No interval found for trip_id: bc90da37651da9ef02c559acedc51c90bd94402cc2a88c7a859fe20d52835494\n",
      "No interval found for trip_id: 767525b38128f755511b15ab6d91dca4b7d350ba8666c6fdb6cdeef0eafc8a62\n",
      "No interval found for trip_id: 1d309b4f519ddd9b30bbaa93728b455b0161d030e6716117d2724f14c42f96d1\n",
      "No interval found for trip_id: fe8b19d7d3c69a7e67ff58ad133ca778feb8268cab1996605ef2c87c54668ab5\n",
      "No interval found for trip_id: 9b4a7167773cb20c6960d78d6f913b57474890b734a6fb33e8d97239732653b9\n",
      "No interval found for trip_id: 02a688969ed198712df786e35fbe680ab3e4b42772a421984b195825b79391ca\n",
      "No interval found for trip_id: 4020ffc31d83b0006ddc9c8f6eb7457f2408e69649a6e2c777d890e22f15a932\n",
      "No interval found for trip_id: 91f4d727b3f014644ccd168a998efe41eb367b7ddbc9a474265a692236255bc6\n",
      "No interval found for trip_id: 93f736032283544accbea881334702b7dce630809ce5ffffb02b520d3f41c7d1\n",
      "No interval found for trip_id: ca154d1a701bc245f476bc84860fa11cce77372b90fecfb4ce8c8c0e2ef4bb60\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def adjust_gaps_with_original_pattern(original_gaps, new_gaps):\n",
    "    \"\"\"\n",
    "    Adjust new gaps based on the pattern of change in the original gaps.\n",
    "    The new gaps will start from 0 but follow the same incremental differences.\n",
    "    \"\"\"\n",
    "    if len(original_gaps) <= 1 or len(new_gaps) == 0:\n",
    "        return new_gaps  # No meaningful adjustment needed\n",
    "\n",
    "    # Calculate the incremental changes (differences) in the original gaps\n",
    "    original_diffs = np.diff(original_gaps)\n",
    "\n",
    "    # Start the new gaps from 0\n",
    "    adjusted_gaps = [0]\n",
    "\n",
    "    # Apply the original differences to the new gaps\n",
    "    for i in range(1, len(new_gaps)):\n",
    "        diff = original_diffs[(i - 1) % len(original_diffs)]\n",
    "        adjusted_gaps.append(adjusted_gaps[-1] + diff)\n",
    "\n",
    "    return adjusted_gaps\n",
    "\n",
    "def segment_trips(df, longest_intervals_df):\n",
    "    road_list = []\n",
    "    second_segment_list = []\n",
    "\n",
    "    # Iterate over each entry in the df dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        interval = longest_intervals_df[longest_intervals_df['trip_id'] == trip_id]\n",
    "        lats = row['lats']\n",
    "        lngs = row['lngs']\n",
    "        time_gap = row['time_gap']\n",
    "        dist_gap = row['dist_gap']\n",
    "        month = row['month']\n",
    "\n",
    "        if not interval.empty:\n",
    "            start_idx = interval['start_idx'].values[0]\n",
    "            end_idx = interval['end_idx'].values[0]\n",
    "\n",
    "            # First segment\n",
    "            first_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': time_gap[:start_idx],\n",
    "                'dist': dist_gap[start_idx - 1] if start_idx > 0 else 0,\n",
    "                'trip_time': time_gap[start_idx - 1] if start_idx > 0 else 0,\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID'],\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': dist_gap[:start_idx],\n",
    "                'lats': lats[:start_idx],\n",
    "                'lngs': lngs[:start_idx],\n",
    "                'month': month,\n",
    "                'time_offset': 0,\n",
    "                'segmentID': 1\n",
    "            }\n",
    "\n",
    "            # Second segment (stationary)\n",
    "            stationary_time_gap = time_gap[start_idx:end_idx + 1]\n",
    "            stationary_dist_gap = dist_gap[start_idx:end_idx + 1]\n",
    "\n",
    "            stationary_time_gap = [time - stationary_time_gap[0] for time in stationary_time_gap]\n",
    "            stationary_dist_gap = [dist - stationary_dist_gap[0] for dist in stationary_dist_gap]\n",
    "\n",
    "            second_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': stationary_time_gap,\n",
    "                'dist': stationary_dist_gap[-1] if len(stationary_dist_gap) > 0 else 0,\n",
    "                'trip_time': stationary_time_gap[-1] if len(stationary_time_gap) > 0 else 0,\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID'],\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': stationary_dist_gap,\n",
    "                'lats': lats[start_idx:end_idx + 1],\n",
    "                'lngs': lngs[start_idx:end_idx + 1],\n",
    "                'month': month,\n",
    "                'time_offset': time_gap[start_idx],\n",
    "                'merchant': row['merchant_id']\n",
    "            }\n",
    "\n",
    "            # Third segment (after stationary)\n",
    "            new_time_gap = time_gap[end_idx + 1:]\n",
    "            new_dist_gap = dist_gap[end_idx + 1:]\n",
    "\n",
    "            # Adjust the gaps using the pattern from the original trip\n",
    "            adjusted_time_gap = adjust_gaps_with_original_pattern(time_gap[end_idx + 1:], new_time_gap)\n",
    "            adjusted_dist_gap = adjust_gaps_with_original_pattern(dist_gap[end_idx + 1:], new_dist_gap)\n",
    "            time_offset3 = time_gap[end_idx + 1] if len(time_gap) > end_idx + 1 else 0\n",
    "\n",
    "            third_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': adjusted_time_gap,\n",
    "                'dist': adjusted_dist_gap[-1] if len(adjusted_dist_gap) > 0 else 0,\n",
    "                'trip_time': adjusted_time_gap[-1] if len(adjusted_time_gap) > 0 else 0,\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID']+round((time_offset3/60),1),\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': adjusted_dist_gap,\n",
    "                'lats': lats[end_idx + 1:],\n",
    "                'lngs': lngs[end_idx + 1:],\n",
    "                'month': month,\n",
    "                'time_offset': time_offset3,\n",
    "                'segmentID': 3\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No interval found for trip_id: {trip_id}\")\n",
    "\n",
    "        # Add segments to their respective lists\n",
    "        if len(first_segment['time_gap']) > 1:\n",
    "            road_list.append(first_segment)\n",
    "\n",
    "        if len(third_segment['time_gap']) > 1:\n",
    "            road_list.append(third_segment)\n",
    "\n",
    "        if len(second_segment['time_gap']) > 1:\n",
    "            second_segment_list.append(second_segment)\n",
    "\n",
    "    # Convert the lists to DataFrames\n",
    "    road_df = pd.DataFrame(road_list)\n",
    "    second_segment_df = pd.DataFrame(second_segment_list)\n",
    "\n",
    "    return road_df, second_segment_df\n",
    "\n",
    "road_df, second_segment_df = segment_trips(df, longest_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_27.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_27.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_6.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_6.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_17.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_17.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_16.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_16.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_19.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_19.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_7.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_7.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_5.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_5.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_17.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_17.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_10.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_10.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_28.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_28.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_22.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_22.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_21.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_21.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_4.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_4.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_15.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_15.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_18.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_18.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_20.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_20.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_2.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_2.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_3.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_3.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_16.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_16.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_26.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_26.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_3.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_3.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_8.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_8.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_6.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_6.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_13.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_13.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_30.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_30.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_29.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_29.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_24.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_24.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_12.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_12.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_25.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_25.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_1.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_1.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_10.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_10.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_27.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_27.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_11.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_11.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_4.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_4.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_22.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_22.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_26.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_26.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_12.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_12.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_11.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_11.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_23.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_23.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_2.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_2.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_14.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_14.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_23.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_23.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_31.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_31.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_5.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_5.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_20.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_20.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_25.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_25.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_7.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_7.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_19.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_19.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_15.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_15.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_21.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_21.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_1.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_1.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_9.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_9.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_9.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_9.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_28.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_28.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_8.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_8.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_24.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_24.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_14.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_14.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_29.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_29.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_01_18.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_01_18.json'\n",
      "Segments 1 & 3 saved to 'segmented_trips/Segmented_Trips_02_13.json'\n",
      "Segment 2 with wait times saved to 'segmented_trips/Merchants_Segments_02_13.json'\n"
     ]
    }
   ],
   "source": [
    "# Remove trips with single 3rd segment occurences\n",
    "single_occurrence_trips = road_df['trip_id'].value_counts()\n",
    "single_occurrence_trips = single_occurrence_trips[single_occurrence_trips == 1].index\n",
    "\n",
    "filtered_single_trips_df = road_df[road_df['trip_id'].isin(single_occurrence_trips)]\n",
    "\n",
    "filtered_single_trips_segment3_df = filtered_single_trips_df[filtered_single_trips_df['segmentID'] == 1]\n",
    "\n",
    "# Remove these trips from road_df\n",
    "road_df = road_df[~road_df['trip_id'].isin(filtered_single_trips_segment3_df['trip_id'])]\n",
    "\n",
    "# Extract unique combinations of dateID and month from road_df\n",
    "unique_date_month_combinations = road_df[['dateID', 'month']].drop_duplicates()\n",
    "\n",
    "# Iterate over each unique combination\n",
    "for _, row in unique_date_month_combinations.iterrows():\n",
    "    date_id = row['dateID']\n",
    "    month = row['month']\n",
    "    \n",
    "    # Filter road_df and second_segment_df for the current dateID and month\n",
    "    road_df_filtered = road_df[(road_df['dateID'] == date_id) & (road_df['month'] == month)]\n",
    "    second_segment_df_filtered = second_segment_df[(second_segment_df['dateID'] == date_id) & (second_segment_df['month'] == month)]\n",
    "    \n",
    "    # Convert to JSON format\n",
    "    road_json = road_df_filtered.to_dict(orient='records')\n",
    "    second_seg_json = second_segment_df_filtered.to_dict(orient='records')\n",
    "\n",
    "    # Generate filenames with month and dateID + 1\n",
    "    road_file_name = f'segmented_trips/Segmented_Trips_0{month}_{date_id + 1}.json'\n",
    "    segment_file_name = f'segmented_trips/Merchants_Segments_0{month}_{date_id + 1}.json'\n",
    "\n",
    "    # Save files\n",
    "    with open(road_file_name, 'w') as file:\n",
    "        for json_obj in road_json:\n",
    "            json.dump(json_obj, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "    with open(segment_file_name, 'w') as file:\n",
    "        for json_obj in second_seg_json:\n",
    "            json.dump(json_obj, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "    print(f\"Segments 1 & 3 saved to '{road_file_name}'\")\n",
    "    print(f\"Segment 2 with wait times saved to '{segment_file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>weekID</th>\n",
       "      <th>timeID</th>\n",
       "      <th>dateID</th>\n",
       "      <th>month</th>\n",
       "      <th>time_offset</th>\n",
       "      <th>segmentID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>499600.000000</td>\n",
       "      <td>499600.000000</td>\n",
       "      <td>499600.000000</td>\n",
       "      <td>499600.000000</td>\n",
       "      <td>499600.000000</td>\n",
       "      <td>499600.000000</td>\n",
       "      <td>499600.000000</td>\n",
       "      <td>499600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.317217</td>\n",
       "      <td>537.415412</td>\n",
       "      <td>2.871511</td>\n",
       "      <td>876.788028</td>\n",
       "      <td>14.962560</td>\n",
       "      <td>1.471910</td>\n",
       "      <td>418.560200</td>\n",
       "      <td>2.120997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>32.353432</td>\n",
       "      <td>441.039350</td>\n",
       "      <td>1.944810</td>\n",
       "      <td>281.038293</td>\n",
       "      <td>8.837051</td>\n",
       "      <td>0.499211</td>\n",
       "      <td>491.149592</td>\n",
       "      <td>0.992654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.761955</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>672.400000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.331419</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>894.100000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.699662</td>\n",
       "      <td>775.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1097.100000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6807.365422</td>\n",
       "      <td>9684.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1440.100000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6442.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dist      trip_time         weekID         timeID  \\\n",
       "count  499600.000000  499600.000000  499600.000000  499600.000000   \n",
       "mean        4.317217     537.415412       2.871511     876.788028   \n",
       "std        32.353432     441.039350       1.944810     281.038293   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.761955     195.000000       1.000000     672.400000   \n",
       "50%         2.331419     423.000000       3.000000     894.100000   \n",
       "75%         5.699662     775.000000       4.000000    1097.100000   \n",
       "max      6807.365422    9684.000000       6.000000    1440.100000   \n",
       "\n",
       "              dateID          month    time_offset      segmentID  \n",
       "count  499600.000000  499600.000000  499600.000000  499600.000000  \n",
       "mean       14.962560       1.471910     418.560200       2.120997  \n",
       "std         8.837051       0.499211     491.149592       0.992654  \n",
       "min         0.000000       1.000000       0.000000       1.000000  \n",
       "25%         7.000000       1.000000       0.000000       1.000000  \n",
       "50%        15.000000       1.000000     304.000000       3.000000  \n",
       "75%        23.000000       2.000000     711.000000       3.000000  \n",
       "max        30.000000       2.000000    6442.000000       3.000000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "road_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>weekID</th>\n",
       "      <th>timeID</th>\n",
       "      <th>dateID</th>\n",
       "      <th>month</th>\n",
       "      <th>time_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>280669.0</td>\n",
       "      <td>280669.000000</td>\n",
       "      <td>280669.000000</td>\n",
       "      <td>280669.000000</td>\n",
       "      <td>280669.000000</td>\n",
       "      <td>280669.000000</td>\n",
       "      <td>280669.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>517.142053</td>\n",
       "      <td>2.866227</td>\n",
       "      <td>871.998511</td>\n",
       "      <td>14.907104</td>\n",
       "      <td>1.475756</td>\n",
       "      <td>216.236396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>368.269721</td>\n",
       "      <td>1.949826</td>\n",
       "      <td>282.633940</td>\n",
       "      <td>8.800414</td>\n",
       "      <td>0.499413</td>\n",
       "      <td>238.066241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>155.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>686.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1094.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5824.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1437.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4519.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dist      trip_time         weekID         timeID         dateID  \\\n",
       "count  280669.0  280669.000000  280669.000000  280669.000000  280669.000000   \n",
       "mean        0.0     517.142053       2.866227     871.998511      14.907104   \n",
       "std         0.0     368.269721       1.949826     282.633940       8.800414   \n",
       "min         0.0      10.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.0     248.000000       1.000000     666.000000       7.000000   \n",
       "50%         0.0     416.000000       3.000000     891.000000      15.000000   \n",
       "75%         0.0     686.000000       5.000000    1094.000000      23.000000   \n",
       "max         0.0    5824.000000       6.000000    1437.000000      30.000000   \n",
       "\n",
       "               month    time_offset  \n",
       "count  280669.000000  280669.000000  \n",
       "mean        1.475756     216.236396  \n",
       "std         0.499413     238.066241  \n",
       "min         1.000000       0.000000  \n",
       "25%         1.000000      40.000000  \n",
       "50%         1.000000     155.000000  \n",
       "75%         2.000000     310.000000  \n",
       "max         2.000000    4519.000000  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_segment_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
