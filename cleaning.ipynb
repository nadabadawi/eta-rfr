{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pings to trajectories json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to json_traj/traj_fix_dist_2024-01-28.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "\n",
    "input_dir = 'careems_data/'\n",
    "output_dir = 'json_traj/'\n",
    "\n",
    "# Get a list of all input files in the directory\n",
    "input_files = [f for f in os.listdir(input_dir) if f.startswith('ooling_pings_jan_24_amman_2024-01-28') and f.endswith('.csv')]\n",
    "\n",
    "#extract the date from filename\n",
    "def extract_date_from_filename(filename):\n",
    "    #regular expression to extract the date in the format YYYY-MM-DD\n",
    "    match = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", filename)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(0)  #return extracted date\n",
    "    else:\n",
    "        raise ValueError(\"Date not found in filename. Expected format: trajectories-YYYY-MM-DD.csv\")\n",
    "\n",
    "#get day of the week from date string\n",
    "def day_of_week(date_str):\n",
    "\n",
    "    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "    #get day of the week (Monday is 0, Sunday is 6)\n",
    "    day_index = date.weekday()\n",
    "\n",
    "    days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "    #get day of the week from the index\n",
    "    day_name = days[day_index]\n",
    "\n",
    "     #get day of the month (from 0 to 30)\n",
    "    day_of_month = date.day - 1  \n",
    "\n",
    "    return day_index, day_name, day_of_month\n",
    "\n",
    "#get time ID (minute of the day from 0 to 1439)\n",
    "def time_id_from_timestamp(timestamp_str):\n",
    "    time = datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    total_minutes = time.hour * 60 + time.minute\n",
    "\n",
    "    return total_minutes\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    #convert to datetime\n",
    "    df['location_read_at'] = pd.to_datetime(df['location_read_at'])\n",
    "\n",
    "    #calculate distance between two points\n",
    "    def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "        return geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "\n",
    "    #calculate time difference in seconds\n",
    "    def calculate_time_difference(time1, time2):\n",
    "        return (time2 - time1).total_seconds()\n",
    "\n",
    "    trip_data = []\n",
    "\n",
    "    null_booking_id = '9b2d5b4678781e53038e91ea5324530a03f27dc1d0e5f6c9bc9d493a23be9de0'\n",
    "    filtered_df = df[df['hash_booking_id'] != null_booking_id]  #filter out the null booking id\n",
    "    grouped = filtered_df.groupby('hash_booking_id')\n",
    "\n",
    "    for booking_id, group in grouped:\n",
    "        #sort pings by timestamp\n",
    "        group = group.sort_values(by='location_read_at')\n",
    "\n",
    "        driver_id = group['hash_driver_id'].iloc[-1]\n",
    "\n",
    "        #first instance of driver id to track switches\n",
    "        first_instance = group[group['hash_driver_id'] == driver_id].iloc[0]\n",
    "\n",
    "        time_id = first_instance['location_read_at']\n",
    "\n",
    "        #filter out pings before switch\n",
    "        valid_group = group[group['location_read_at'] >= time_id]\n",
    "\n",
    "        lngs = valid_group['longitude'].tolist()\n",
    "        lats = valid_group['latitude'].tolist()\n",
    "\n",
    "        #dist gaps\n",
    "        dist_gaps = [0]\n",
    "        time_gaps = [0]\n",
    "        prev_lat = lats[0]\n",
    "        prev_lng = lngs[0]\n",
    "        prev_time = time_id\n",
    "        cum_dist = 0\n",
    "\n",
    "        #total distance\n",
    "        for lat, lng in zip(lats[1:], lngs[1:]):\n",
    "            dist = calculate_distance(prev_lat, prev_lng, lat, lng)\n",
    "            cum_dist += dist\n",
    "            dist_gaps.append(cum_dist)\n",
    "            prev_lat = lat\n",
    "            prev_lng = lng\n",
    "\n",
    "        total_dist = cum_dist\n",
    "\n",
    "        #time gaps\n",
    "        time_gaps = [(t - time_id).total_seconds() for t in valid_group['location_read_at']]\n",
    "\n",
    "        #total time\n",
    "        total_time = calculate_time_difference(valid_group['location_read_at'].iloc[0], valid_group['location_read_at'].iloc[-1])\n",
    "\n",
    "        trip_data.append([booking_id, driver_id, time_id, lngs, lats, total_dist, total_time, time_gaps, dist_gaps])\n",
    "\n",
    "    output_df = pd.DataFrame(trip_data, columns=['booking_id', 'driver_id', 'time_id', 'lngs', 'lats', 'dist', 'time', 'time_gap', 'dist_gap'])\n",
    "\n",
    "    # Convert DataFrame to list of dicts for JSON processing\n",
    "    trip_data_dicts = output_df.to_dict('records')\n",
    "\n",
    "    # Create JSON objects\n",
    "    json_data = []\n",
    "    date_str = extract_date_from_filename(input_file)\n",
    "\n",
    "    for row in trip_data_dicts:\n",
    "        # Day of the week\n",
    "        week_id, name, date_id = day_of_week(date_str)\n",
    "        # Time ID is minute of day\n",
    "        time_id = time_id_from_timestamp(str(row['time_id']))\n",
    "\n",
    "        new_dict = {\n",
    "            'trip_id': row['booking_id'],\n",
    "            'time_gap': row['time_gap'],\n",
    "            'dist': float(row['dist']),\n",
    "            'lats': row['lats'],\n",
    "            'driverID': row['driver_id'],\n",
    "            'weekID': week_id,\n",
    "            'timeID': time_id,\n",
    "            'dateID': date_id,\n",
    "            'time': float(row['time']),\n",
    "            'lngs': row['lngs'],\n",
    "            'dist_gap': row['dist_gap']\n",
    "        }\n",
    "        json_str = json.dumps(new_dict, separators=(',', ':'))  # Convert to JSON string\n",
    "        json_data.append(json_str)\n",
    "\n",
    "    # Write JSON data to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as output_file:\n",
    "        for entry in json_data:\n",
    "            output_file.write(entry + '\\n')\n",
    "            output_file.flush()\n",
    "\n",
    "# Process each input file\n",
    "for input_file in input_files:\n",
    "    date_str = input_file.split('_')[-1].split('.')[0]  # Extract date from file name\n",
    "    output_file = os.path.join(output_dir, f'traj_fix_dist_{date_str}.json')\n",
    "    process_file(os.path.join(input_dir, input_file), output_file)\n",
    "    print(f\"Data has been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and cleaned data for date: 2024-01-22\n",
      "Processed and cleaned data for date: 2024-01-11\n",
      "Processed and cleaned data for date: 2024-01-23\n",
      "Processed and cleaned data for date: 2024-01-01\n",
      "Processed and cleaned data for date: 2024-01-21\n",
      "Processed and cleaned data for date: 2024-01-06\n",
      "Processed and cleaned data for date: 2024-01-12\n",
      "Processed and cleaned data for date: 2024-01-05\n",
      "Processed and cleaned data for date: 2024-01-17\n",
      "Processed and cleaned data for date: 2024-01-24\n",
      "Processed and cleaned data for date: 2024-01-09\n",
      "Processed and cleaned data for date: 2024-01-15\n",
      "Processed and cleaned data for date: 2024-01-13\n",
      "Processed and cleaned data for date: 2024-01-04\n",
      "Processed and cleaned data for date: 2024-01-14\n",
      "Processed and cleaned data for date: 2024-01-16\n",
      "Processed and cleaned data for date: 2024-01-03\n",
      "Processed and cleaned data for date: 2024-01-18\n",
      "Processed and cleaned data for date: 2024-01-08\n",
      "Processed and cleaned data for date: 2024-01-19\n",
      "Processed and cleaned data for date: 2024-01-02\n",
      "Processed and cleaned data for date: 2024-01-20\n",
      "Processed and cleaned data for date: 2024-01-10\n",
      "Processed and cleaned data for date: 2024-01-07\n",
      "Matching process completed. Check the 'trial week/clean_data/' directory for results.\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'json_traj/jan/'\n",
    "output_dir = 'clean_data/'\n",
    "clean_file_template = \"clean_{date}.json\"\n",
    "pooling_data = pd.read_csv(\"careems data/anon_pooling_jan_24_amman.csv\")\n",
    "\n",
    "\n",
    "for j in os.listdir(input_dir):\n",
    "    date_str = j.split('_')[1].split('.')[0]  # Extract date from file name\n",
    "    filtered_pooling_data = pooling_data[pooling_data['day'] == date_str]\n",
    "    \n",
    "    if filtered_pooling_data.empty:\n",
    "        print(f\"No pooling data for date: {date_str}\")\n",
    "        continue\n",
    "\n",
    "    input_file_path = os.path.join(input_dir, j)\n",
    "    clean_file_path = os.path.join(output_dir, clean_file_template.format(date=date_str))\n",
    "\n",
    "    with open(input_file_path, \"r\") as json_traj_file:\n",
    "        new_data = [json.loads(line) for line in json_traj_file]  # Read each line as a JSON object\n",
    "\n",
    "    with open(clean_file_path, 'w', encoding='utf-8') as clean_file:\n",
    "        for entry in new_data:\n",
    "            entry_time = entry[\"time\"]\n",
    "            entry_trip_id = entry[\"trip_id\"]\n",
    "            entry_driver_id = entry[\"driverID\"]\n",
    "\n",
    "            for _, row in filtered_pooling_data.iterrows():\n",
    "                pool_time = float(row[\"captain_engagement_time\"] * 60)\n",
    "                pool_trip_id = row[\"booking_id\"]\n",
    "                pool_driver_id = row[\"captain_id\"]\n",
    "                time_diff = pool_time - entry_time\n",
    "\n",
    "                # Adding time difference to entry\n",
    "                entry_with_time_diff = entry.copy()\n",
    "                entry_with_time_diff[\"time_diff\"] = time_diff\n",
    "\n",
    "                # Writing good trips to clean file (same trip and driver id and <= 5 sec time diff)\n",
    "                if entry_trip_id == pool_trip_id and entry_driver_id == pool_driver_id and abs(time_diff) <= 5:\n",
    "                    json.dump(entry_with_time_diff, clean_file)\n",
    "                    clean_file.write(\"\\n\")\n",
    "                    break\n",
    "\n",
    "    print(f\"Processed and cleaned data for date: {date_str}\")\n",
    "\n",
    "print(\"Matching process completed. Check the 'trial week/clean_data/' directory for results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map driver ids across all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'clean_data/'  # Replace with the path to your folder\n",
    "\n",
    "# Get a list of all JSON files in the folder\n",
    "input_files = glob.glob(os.path.join(folder_path, '*.json'))\n",
    "\n",
    "# Step 1: Collect unique driver IDs from all files\n",
    "driver_ids = set()  # Use a set to ensure uniqueness\n",
    "for file_path in input_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as input_file:\n",
    "        for line in input_file:\n",
    "            entry = json.loads(line)\n",
    "            driver_ids.add(entry['driverID'])\n",
    "\n",
    "# Step 2: Create a mapping from unique driver IDs to integers\n",
    "driver_id_map = {driver_id: idx for idx, driver_id in enumerate(driver_ids, start=1)}\n",
    "\n",
    "# Step 3: Apply the mapping and add a new entry to the data\n",
    "for file_path in input_files:\n",
    "    mapped_data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as input_file:\n",
    "        for line in input_file:\n",
    "            entry = json.loads(line)\n",
    "            entry['mapped_driveID'] = driver_id_map[entry['driverID']]  # Add new entry\n",
    "            mapped_data.append(entry)\n",
    "    \n",
    "    # Step 4: Write the modified data back to the original file\n",
    "    with open(file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for entry in mapped_data:\n",
    "            json_str = json.dumps(entry, separators=(',', ':'))  # Convert to single-line JSON\n",
    "            output_file.write(json_str + '\\n')  # Write each entry to a new line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change names of driverid and mapped driver id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Specify the folder containing the JSON files\n",
    "folder_path = 'clean_data/'  # Replace with the path to your folder\n",
    "\n",
    "# Get a list of all JSON files in the folder\n",
    "input_files = glob.glob(os.path.join(folder_path, '*.json'))\n",
    "\n",
    "# Process each file in the folder\n",
    "for file_path in input_files:\n",
    "    updated_data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as input_file:\n",
    "        for line in input_file:\n",
    "            entry = json.loads(line)\n",
    "            # Rename driverID to unmapped_driverID\n",
    "            entry['unmapped_driverID'] = entry.pop('driverID')\n",
    "            # Rename mapped_driver_id to driverID\n",
    "            entry['driverID'] = entry.pop('mapped_driveID')\n",
    "            updated_data.append(entry)\n",
    "    \n",
    "    # Write the modified data back to the original file\n",
    "    with open(file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for entry in updated_data:\n",
    "            json_str = json.dumps(entry, separators=(',', ':'))  # Convert to single-line JSON\n",
    "            output_file.write(json_str + '\\n')  # Write each entry to a new line\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
