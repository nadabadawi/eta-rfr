{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "from shapely.ops import unary_union\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/linaserry/Desktop/thesis/eta/eta-rfr/careem_rfr_v2.2.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/linaserry/Desktop/thesis/eta/eta-rfr/careem_rfr_v2.2.ipynb#Y222sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m os\u001b[39m.\u001b[39mmkdir(\u001b[39m'\u001b[39m\u001b[39mTrip Splitting\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.mkdir('Trip Splitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory and file name pattern\n",
    "directory = 'Data/clean_data/\n",
    "file_base = 'clean_2024-01-'\n",
    "\n",
    "# Generate the list of file names\n",
    "files_names = [f'{file_base}{i}.json' for i in range(25, 32)] # last week of January \n",
    "\n",
    "# Generate the list of file directories\n",
    "files = [os.path.join(directory, file_name) for file_name in files_names]\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from files and drop time_diff column\n",
    "for file_name in files:\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "if 'time_diff' in df.columns:\n",
    "    df.drop('time_diff', axis=1, inplace=True)\n",
    "\n",
    "df = df.rename(columns={'time': 'trip_time'})\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pooling data\n",
    "pooling_file_path = 'Data/careems_cata/anon_pooling_jan_24_amman.csv' \n",
    "df_pooling = pd.read_csv(pooling_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merchant data\n",
    "merchant_file_path = 'Data/careems_cata/order_merchant_id_anon.parquet'\n",
    "merchants_df = pd.read_parquet(merchant_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left join merchants on the pings df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Merge merchants_df with df_pooling on 'order_id'\n",
    "merged_pooling = df_pooling.merge(merchants_df[['order_id', 'merchant_id']], on='order_id', how='left')\n",
    "\n",
    "# Merge with df on 'trip_id' \n",
    "df = df.merge(merged_pooling[['booking_id', 'merchant_id']], left_on='trip_id', right_on='booking_id', how='left')\n",
    "\n",
    "df = df.drop(columns=['booking_id'])\n",
    "\n",
    "# Check how many 'merchant_id' values are null \n",
    "print(df['merchant_id'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Stationarity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all stationary intervals over each trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationary intervals saved to stationary_intervals_all_trips.csv\n"
     ]
    }
   ],
   "source": [
    "def detect_stationary_intervals(df):\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = np.array(row['lats'])\n",
    "        lngs = np.array(row['lngs'])\n",
    "        time_gap = np.array(row['time_gap'])\n",
    "        \n",
    "        # Compute differences between consecutive points directly from lat/lng arrays\n",
    "        lat_diff = np.abs(np.diff(lats))\n",
    "        lng_diff = np.abs(np.diff(lngs))\n",
    "\n",
    "        # Identify stationary intervals\n",
    "        stationary_indices = np.where((lat_diff == 0) & (lng_diff == 0))[0]\n",
    "        \n",
    "        # Group consecutive stationary indices into intervals\n",
    "        if len(stationary_indices) > 0:\n",
    "            start_idx = stationary_indices[0]\n",
    "            for i in range(1, len(stationary_indices)):\n",
    "                # If the current index is not consecutive, close the interval\n",
    "                if stationary_indices[i] != stationary_indices[i - 1] + 1:\n",
    "                    end_idx = stationary_indices[i - 1]\n",
    "                    # Save the interval only if it has more than one index\n",
    "                    if end_idx > start_idx:\n",
    "                        interval = {\n",
    "                            \"trip_id\": trip_id,\n",
    "                            \"start_idx\": int(start_idx),\n",
    "                            \"end_idx\": int(end_idx)+1,\n",
    "                            \"start_lat\": float(lats[start_idx]),\n",
    "                            \"start_lng\": float(lngs[start_idx]),\n",
    "                            \"end_lat\": float(lats[end_idx+1]),\n",
    "                            \"end_lng\": float(lngs[end_idx+1]),\n",
    "                            \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                        }\n",
    "                        results.append(interval)\n",
    "                    # Start a new interval\n",
    "                    start_idx = stationary_indices[i]\n",
    "            # Add the last interval if it has more than one index\n",
    "            end_idx = stationary_indices[-1]\n",
    "            if end_idx > start_idx:\n",
    "                interval = {\n",
    "                    \"trip_id\": trip_id,\n",
    "                    \"start_idx\": int(start_idx),\n",
    "                    \"end_idx\": int(end_idx)+1,\n",
    "                    \"start_lat\": float(lats[start_idx]),\n",
    "                    \"start_lng\": float(lngs[start_idx]),\n",
    "                    \"end_lat\": float(lats[end_idx+1]),\n",
    "                    \"end_lng\": float(lngs[end_idx+1]),\n",
    "                    \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                }\n",
    "                results.append(interval)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "stationary_df_pre_truncation = detect_stationary_intervals(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate trips where a stationary interval exists at the end of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated trips saved to 'truncated_trips.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def truncate_trips_if_stationary(df, stationary_df):\n",
    "    truncated_trips = []\n",
    "\n",
    "    # Iterate over each trip in the df\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = row['lats']\n",
    "        lngs = row['lngs']\n",
    "        time_gap = row['time_gap']\n",
    "        dist_gap = row['dist_gap']\n",
    "        dist = row['dist']\n",
    "        trip_time = row['trip_time']\n",
    "\n",
    "        # Get the last coordinates of the trip\n",
    "        last_lat, last_lng = lats[-1], lngs[-1]\n",
    "\n",
    "        # Find the corresponding stationary intervals for this trip\n",
    "        intervals = stationary_df[stationary_df['trip_id'] == trip_id]\n",
    "\n",
    "        if not intervals.empty:\n",
    "            # Get the last stationary interval for this trip\n",
    "            last_interval = intervals.iloc[-1]\n",
    "\n",
    "            interval_lat, interval_lng = last_interval['end_lat'], last_interval['end_lng']\n",
    "            time_elapsed = last_interval['time_elapsed']\n",
    "\n",
    "            # Check if the last stationary interval's coordinates match the last trip coordinates\n",
    "            if (last_lat == interval_lat and last_lng == interval_lng) and time_elapsed > 10:\n",
    "                # Keep only the first part of the stationary segment\n",
    "                truncated_trip = {\n",
    "                    'trip_id': trip_id,\n",
    "                    'lats': lats[:last_interval['start_idx'] + 1],\n",
    "                    'lngs': lngs[:last_interval['start_idx'] + 1],\n",
    "                    'time_gap': time_gap[:last_interval['start_idx'] + 1],\n",
    "                    'dist_gap': dist_gap[:last_interval['start_idx'] + 1],\n",
    "                    'dist': dist,  # Keep the original distance\n",
    "                    'trip_time': time_gap[last_interval['start_idx']],  # Adjusted trip time\n",
    "                    'driverID': row['driverID'],\n",
    "                    'weekID': row['weekID'],\n",
    "                    'timeID': row['timeID'],\n",
    "                    'dateID': row['dateID'],\n",
    "                    'merchant_id': row['merchant_id']\n",
    "                }\n",
    "                truncated_trips.append(truncated_trip)\n",
    "            else:\n",
    "                # If no truncation is needed, keep the original trip\n",
    "                truncated_trips.append(row.to_dict())\n",
    "        else:\n",
    "            # If no stationary intervals exist, keep the original trip\n",
    "            truncated_trips.append(row.to_dict())\n",
    "\n",
    "    # Convert the list of truncated trips back into a df\n",
    "    truncated_df = pd.DataFrame(truncated_trips)\n",
    "\n",
    "    # Ensure all columns match the original schema\n",
    "    for column in df.columns:\n",
    "        if column not in truncated_df.columns:\n",
    "            truncated_df[column] = None  # Add missing columns with default None values\n",
    "\n",
    "    # Reorder columns to match the original DataFrame\n",
    "    truncated_df = truncated_df[df.columns]\n",
    "\n",
    "    return truncated_df\n",
    "\n",
    "truncated_df = truncate_trips_if_stationary(df, stationary_df_pre_truncation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find new stationary intervals post truncation and longest interval for each trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationary intervals saved to stationary_intervals_all_trips_truncated.csv\n"
     ]
    }
   ],
   "source": [
    "def detect_stationary_intervals(df):\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = np.array(row['lats'])\n",
    "        lngs = np.array(row['lngs'])\n",
    "        time_gap = np.array(row['time_gap'])\n",
    "        \n",
    "        # Compute differences between consecutive points directly from lat/lng arrays\n",
    "        lat_diff = np.abs(np.diff(lats))\n",
    "        lng_diff = np.abs(np.diff(lngs))\n",
    "\n",
    "        # Identify stationary intervals \n",
    "        stationary_indices = np.where((lat_diff == 0) & (lng_diff == 0))[0]\n",
    "        \n",
    "        # Group consecutive stationary indices into intervals\n",
    "        if len(stationary_indices) > 0:\n",
    "            start_idx = stationary_indices[0]\n",
    "            for i in range(1, len(stationary_indices)):\n",
    "                # If the current index is not consecutive, close the interval\n",
    "                if stationary_indices[i] != stationary_indices[i - 1] + 1:\n",
    "                    end_idx = stationary_indices[i - 1]\n",
    "                    # Save the interval only if it has more than one index\n",
    "                    if end_idx > start_idx:\n",
    "                        interval = {\n",
    "                            \"trip_id\": trip_id,\n",
    "                            \"start_idx\": int(start_idx),\n",
    "                            \"end_idx\": int(end_idx)+1,\n",
    "                            \"start_lat\": float(lats[start_idx]),\n",
    "                            \"start_lng\": float(lngs[start_idx]),\n",
    "                            \"end_lat\": float(lats[end_idx+1]),\n",
    "                            \"end_lng\": float(lngs[end_idx+1]),\n",
    "                            \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                        }\n",
    "                        results.append(interval)\n",
    "                    # Start a new interval\n",
    "                    start_idx = stationary_indices[i]\n",
    "            # Add the last interval if it has more than one index\n",
    "            end_idx = stationary_indices[-1]\n",
    "            if end_idx > start_idx:\n",
    "                interval = {\n",
    "                    \"trip_id\": trip_id,\n",
    "                    \"start_idx\": int(start_idx),\n",
    "                    \"end_idx\": int(end_idx)+1,\n",
    "                    \"start_lat\": float(lats[start_idx]),\n",
    "                    \"start_lng\": float(lngs[start_idx]),\n",
    "                    \"end_lat\": float(lats[end_idx+1]),\n",
    "                    \"end_lng\": float(lngs[end_idx+1]),\n",
    "                    \"time_elapsed\": float(time_gap[end_idx+1] - time_gap[start_idx])\n",
    "                }\n",
    "                results.append(interval)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "stationary_df = detect_stationary_intervals(truncated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=truncated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_longest_stationary_interval(stationary_df):\n",
    "    # Find the longest stationary interval for each trip_id based on time_elapsed\n",
    "    longest_intervals = stationary_df.loc[stationary_df.groupby('trip_id')['time_elapsed'].idxmax()]\n",
    "\n",
    "    longest_intervals.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return longest_intervals\n",
    "\n",
    "longest_intervals_df = extract_longest_stationary_interval(stationary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combined df with pooling coordinates and longest stationary interval coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined(longest_intervals_df, pooling_df):\n",
    "    \n",
    "    # Rename 'booking_id' to 'trip_id' in pooling_df for consistency\n",
    "    pooling_df = pooling_df.rename(columns={'booking_id': 'trip_id'})\n",
    "    \n",
    "    combined_df = (\n",
    "        longest_intervals_df[['trip_id', 'start_lat', 'start_lng']]\n",
    "        .merge(pooling_df[['trip_id', 'pickup_latitude', 'pickup_longitude']], on='trip_id', how='inner')\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity in the combined data\n",
    "    combined_df = combined_df.rename(columns={\n",
    "        'start_lat': 'stationary_df_lat',\n",
    "        'start_lng': 'stationary_df_lng',\n",
    "        'pickup_latitude': 'pooling_df_lat',\n",
    "        'pickup_longitude': 'pooling_df_lng'\n",
    "    })   \n",
    "    return combined_df\n",
    "\n",
    "combined_df = create_combined(longest_intervals_df, df_pooling)\n",
    "\n",
    "combined_df = combined_df.merge(df[['trip_id', 'merchant_id']], on='trip_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update incorrect pooling coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistent groups saved to merchant_lat_lng_mismatch.csv\n",
      "Number of inconsistent groups: 987\n"
     ]
    }
   ],
   "source": [
    "def check_inconsistent_groups_with_tolerance_fix(combined_df):\n",
    "    \n",
    "    # Lists to store inconsistent groups and detailed non-unique counts\n",
    "    inconsistent_groups = []\n",
    "    nonunique_info = []\n",
    "\n",
    "    for merchant_id, group in combined_df.groupby('merchant_id'):\n",
    "        # Extract latitude and longitude values for the group\n",
    "        lat_values = group['pooling_df_lat'].values\n",
    "        lng_values = group['pooling_df_lng'].values\n",
    "\n",
    "        # Count occurrences of each latitude and longitude\n",
    "        lat_counts = pd.Series(lat_values).value_counts()\n",
    "        lng_counts = pd.Series(lng_values).value_counts()\n",
    "\n",
    "        # Identify the majority (most common) latitude and longitude\n",
    "        majority_lat = lat_counts.idxmax()\n",
    "        majority_lng = lng_counts.idxmax()\n",
    "\n",
    "        # Filter out the non-majority coordinates\n",
    "        non_majority_lats = lat_counts[lat_counts.index != majority_lat]\n",
    "        non_majority_lngs = lng_counts[lng_counts.index != majority_lng]\n",
    "\n",
    "        # If there are non-majority coordinates, store the information\n",
    "        if not non_majority_lats.empty or not non_majority_lngs.empty:\n",
    "            inconsistent_groups.append(group)\n",
    "\n",
    "            # Store the non-unique information for this merchant_id\n",
    "            nonunique_info.append({\n",
    "                'merchant_id': merchant_id,\n",
    "                'non_majority_lats': non_majority_lats.to_dict(),  # Non-majority latitudes and their counts\n",
    "                'non_majority_lngs': non_majority_lngs.to_dict(),  # Non-majority longitudes and their counts\n",
    "                'group_size': len(group)\n",
    "            })\n",
    "\n",
    "    # Combine all inconsistent groups into a single DataFrame\n",
    "    if inconsistent_groups:\n",
    "        print(f\"Number of inconsistent groups: {len(inconsistent_groups)}\")\n",
    "        non_majority_df = pd.DataFrame(nonunique_info)\n",
    "    else:\n",
    "        print(\"No inconsistencies found.\")\n",
    "\n",
    "    return non_majority_df\n",
    "\n",
    "non_majority_df = check_inconsistent_groups_with_tolerance_fix(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected 531 trip coordinates.\n"
     ]
    }
   ],
   "source": [
    "# Change incorrect pooling coordinates in combined_df to the majority coordinates for each merchant\n",
    "def correct_pooling_coordinates_with_nonmajority(non_majority_df, combined_df):\n",
    "\n",
    "    # Iterate over each merchant in the non-majority df\n",
    "    for _, row in non_majority_df.iterrows():\n",
    "        merchant_id = row['merchant_id']\n",
    "\n",
    "        # Get the majority latitude and longitude for this merchant from combined_df\n",
    "        majority_lat = combined_df[combined_df['merchant_id'] == merchant_id]['pooling_df_lat'].mode()[0]\n",
    "        majority_lng = combined_df[combined_df['merchant_id'] == merchant_id]['pooling_df_lng'].mode()[0]\n",
    "\n",
    "        # Find the trip_ids with non-majority coordinates\n",
    "        merchant_group = combined_df[combined_df['merchant_id'] == merchant_id]\n",
    "        incorrect_trips = merchant_group[\n",
    "            (~np.isclose(merchant_group['pooling_df_lat'], majority_lat, atol=0.001)) |\n",
    "            (~np.isclose(merchant_group['pooling_df_lng'], majority_lng, atol=0.001))\n",
    "        ]\n",
    "\n",
    "        # Update the coordinates in combined_df for the incorrect trips\n",
    "        for trip_id in incorrect_trips['trip_id']:\n",
    "            combined_df.loc[combined_df['booking_id'] == trip_id, ['pooling_df_lat', 'pooling_df_lat']] = [\n",
    "                majority_lat, majority_lng\n",
    "            ]\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "combined_df = correct_pooling_coordinates_with_nonmajority(non_majority_df, combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract non-matching coordinates and drop them from df and longest stationary df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadab\\AppData\\Local\\Temp\\ipykernel_26592\\226864586.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_matching_df['distance_km'] = haversine(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-matching coordinate entries saved to non_matching_coordinates.csv\n",
      "trip_id              10071\n",
      "stationary_df_lat    10071\n",
      "stationary_df_lng    10071\n",
      "pooling_df_lat       10071\n",
      "pooling_df_lng       10071\n",
      "merchant_id          10071\n",
      "distance_km          10071\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "\n",
    "    # Compute differences\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Apply Haversine formula\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "def extract_non_matching_coordinates(combined_df):\n",
    "    # Identify rows where the stationary coordinates do not match pooling coordinates\n",
    "    non_matching_df = combined_df[\n",
    "        (combined_df['stationary_df_lat'] != combined_df['pooling_df_lat']) |\n",
    "        (combined_df['stationary_df_lng'] != combined_df['pooling_df_lng'])\n",
    "    ]\n",
    "\n",
    "    # calculate the distance between the coordinates\n",
    "    non_matching_df['distance_km'] = haversine(\n",
    "        non_matching_df['stationary_df_lat'],\n",
    "        non_matching_df['stationary_df_lng'],\n",
    "        non_matching_df['pooling_df_lat'],\n",
    "        non_matching_df['pooling_df_lng']\n",
    "    )\n",
    "\n",
    "    return non_matching_df\n",
    "\n",
    "non_matching_entries = extract_non_matching_coordinates(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrames saved.\n"
     ]
    }
   ],
   "source": [
    "# Drop non-matching trips from `df` and `longest_intervals_df`\n",
    "def drop_non_matching_trips(df, longest_intervals_df, non_matching_entries):\n",
    "    # Extract the trip IDs from non-matching entries\n",
    "    non_matching_trip_ids = non_matching_entries['trip_id'].unique()\n",
    "\n",
    "    # Drop these trips from both dfs\n",
    "    df_filtered = df[~df['trip_id'].isin(non_matching_trip_ids)]\n",
    "    longest_intervals_filtered = longest_intervals_df[~longest_intervals_df['trip_id'].isin(non_matching_trip_ids)]\n",
    "\n",
    "    return df_filtered, longest_intervals_filtered\n",
    "\n",
    "\n",
    "df, longest_intervals_df = drop_non_matching_trips(df, longest_intervals_df, non_matching_entries)\n",
    "print(\"Filtered DataFrames saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Splitting trips into 3 segments (Driver-to-Merchant, Wait-Time-at-Merchant, Merchant-to-Customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split trips based on stationary intervals matching coordinates from pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No interval found for trip_id: 25049734158c2944703546bdc2cbe5740170230779e858db275cb6512aaa0047\n",
      "No interval found for trip_id: 4ad4bdf931291e3f2176bef563091893e0bde118f28a2ef5b172ece38949ed38\n",
      "No interval found for trip_id: 5057cb61e831386cd034d24462a4f6ecebecb3606bc2d39d4c991fd04b56e413\n",
      "No interval found for trip_id: 620950188eebce63b6bf5be22cb280dbb8d75b5cef8904b6de30bb892d2a1c3a\n",
      "No interval found for trip_id: 64186edda7362e34c7598fe8af53c01cde6071b3d932380eaa30299419712632\n",
      "No interval found for trip_id: 07a2f3cdf28096c27c4c2b81bcfafe30a96095097f6e96e79878f714a9ec648c\n",
      "No interval found for trip_id: 6299a27e7d370d6904a05073fdd554e0efbf09f0d9458d7e0f677e75644c2e34\n",
      "No interval found for trip_id: 65d99e964c50a5eed233fe9a7864114c1d91cf6b6ba542bbf4906a998d4df27f\n",
      "No interval found for trip_id: 7d71d55894365ccad46d7e7c533997c58e200cc235919b0c817944af8b5226a1\n",
      "No interval found for trip_id: 01ff05c850fb030d8174a48d70a7327747dd0177030bc5cad10f5c9d25b95b33\n",
      "No interval found for trip_id: ec0286dc18471dfaf418922d48c55f7ec72f1e528954b80fc707705137fb5a21\n",
      "No interval found for trip_id: f01f5f9ddc73c41afad46682bccac8a38ccfad813cd19c7bacbde98ae8b96cff\n",
      "No interval found for trip_id: f82b6c7a45db4fd93e76d73881343d2aeeec3fefb876401a4b9380c9ad3cf37d\n",
      "No interval found for trip_id: 5dc0e366867ddce4caf3a1861cd46b216165cbb1c513cd796e53942b12c9eb1a\n",
      "No interval found for trip_id: b61a3a9ada58f81859560718f1697a84aa9543ac6be94e048b54178ea29dac66\n",
      "No interval found for trip_id: bad44c8533c0435d81119b807997f4836a6648da594f5d4da2ece47bc1bed460\n",
      "No interval found for trip_id: e0e7a3c5ceb935f13325f33aada255dc33fb8e7d343dcd21607e2f8606378005\n",
      "No interval found for trip_id: f9a051e1debe7d4dda61a109a99262a6ae77d6cc261a822badd05242f94bc307\n",
      "No interval found for trip_id: 35eb23a5ef4a2ea073a83d60f6c5bfd77111f100688cbf2a2ed9dddc9a90aeba\n",
      "No interval found for trip_id: 553a5a3eac65d331a068a8a60c29ce6c3ce4bc4cff4c63542f2d45cdc62bd55f\n",
      "No interval found for trip_id: 64e730589bbba0cb6c3f5b326825303c12de74cce5039304b8d75c4130834950\n",
      "No interval found for trip_id: 95211438a9e97d1e5ca9185f0a6b2c814e399dc323853b0fb85d0d6918d12ca6\n",
      "No interval found for trip_id: 08d42691a20f566e666116e678f3c56651624a768b7a43092a5245acbbc71ace\n",
      "No interval found for trip_id: 4a364cf873fe027a333ee1ad5be259751515200bca58d5352103e0c7b3306aec\n",
      "No interval found for trip_id: 756619c50b17ee83526dda6c0ee4ead83cf50cdb8b76718630c9f10aa6e09119\n",
      "No interval found for trip_id: a8178a79f9a2b059f4e951777992d4e6c36e3782a5767e4010f0831a46e36d6a\n",
      "No interval found for trip_id: e0933d83f8c0492912d0ba4418433f3114df0d6ae9c320ae2d62cd2099262b8f\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def adjust_gaps_with_original_pattern(original_gaps, new_gaps):\n",
    "    \"\"\"\n",
    "    Adjust new gaps based on the pattern of change in the original gaps.\n",
    "    The new gaps will start from 0 but follow the same incremental differences.\n",
    "    \"\"\"\n",
    "    if len(original_gaps) <= 1 or len(new_gaps) == 0:\n",
    "        return new_gaps  # No meaningful adjustment needed\n",
    "\n",
    "    # Calculate the incremental changes (differences) in the original gaps\n",
    "    original_diffs = np.diff(original_gaps)\n",
    "\n",
    "    # Start the new gaps from 0\n",
    "    adjusted_gaps = [0]\n",
    "\n",
    "    # Apply the original differences to the new gaps\n",
    "    for i in range(1, len(new_gaps)):\n",
    "        diff = original_diffs[(i - 1) % len(original_diffs)]\n",
    "        adjusted_gaps.append(adjusted_gaps[-1] + diff)\n",
    "\n",
    "    return adjusted_gaps\n",
    "\n",
    "def segment_trips(df, longest_intervals_df):\n",
    "    road_list = []\n",
    "    second_segment_list = []\n",
    "\n",
    "    # Iterate over each entry in the df dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        interval = longest_intervals_df[longest_intervals_df['trip_id'] == trip_id]\n",
    "        lats = row['lats']\n",
    "        lngs = row['lngs']\n",
    "        time_gap = row['time_gap']\n",
    "        dist_gap = row['dist_gap']\n",
    "\n",
    "        if not interval.empty:\n",
    "            start_idx = interval['start_idx'].values[0]\n",
    "            end_idx = interval['end_idx'].values[0]\n",
    "\n",
    "            # First segment\n",
    "            first_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': time_gap[:start_idx],\n",
    "                'dist': dist_gap[start_idx - 1] if start_idx > 0 else 0,\n",
    "                'trip_time': time_gap[start_idx - 1] if start_idx > 0 else 0,\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID'],\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': dist_gap[:start_idx],\n",
    "                'lats': lats[:start_idx],\n",
    "                'lngs': lngs[:start_idx],\n",
    "                'time_offset': 0,\n",
    "                'segmentID': 1\n",
    "            }\n",
    "\n",
    "            # Second segment (stationary)\n",
    "            stationary_time_gap = time_gap[start_idx:end_idx + 1]\n",
    "            stationary_dist_gap = dist_gap[start_idx:end_idx + 1]\n",
    "\n",
    "            stationary_time_gap = [time - stationary_time_gap[0] for time in stationary_time_gap]\n",
    "            stationary_dist_gap = [dist - stationary_dist_gap[0] for dist in stationary_dist_gap]\n",
    "\n",
    "            second_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': stationary_time_gap,\n",
    "                'dist': stationary_dist_gap[-1] if len(stationary_dist_gap) > 0 else 0,\n",
    "                'trip_time': stationary_time_gap[-1] if len(stationary_time_gap) > 0 else 0,\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID'],\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': stationary_dist_gap,\n",
    "                'lats': lats[start_idx:end_idx + 1],\n",
    "                'lngs': lngs[start_idx:end_idx + 1],\n",
    "                'time_offset': time_gap[start_idx],\n",
    "                'merchant': row['merchant_id']\n",
    "            }\n",
    "\n",
    "            # Third segment (after stationary)\n",
    "            new_time_gap = time_gap[end_idx + 1:]\n",
    "            new_dist_gap = dist_gap[end_idx + 1:]\n",
    "\n",
    "            # Adjust the gaps using the pattern from the original trip\n",
    "            adjusted_time_gap = adjust_gaps_with_original_pattern(time_gap[end_idx + 1:], new_time_gap)\n",
    "            adjusted_dist_gap = adjust_gaps_with_original_pattern(dist_gap[end_idx + 1:], new_dist_gap)\n",
    "            time_offset3 = time_gap[end_idx + 1] if len(time_gap) > end_idx + 1 else 0\n",
    "\n",
    "            third_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': adjusted_time_gap,\n",
    "                'dist': adjusted_dist_gap[-1] if len(adjusted_dist_gap) > 0 else 0,\n",
    "                'trip_time': adjusted_time_gap[-1] if len(adjusted_time_gap) > 0 else 0,\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID']+round((time_offset3/60),1),\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': adjusted_dist_gap,\n",
    "                'lats': lats[end_idx + 1:],\n",
    "                'lngs': lngs[end_idx + 1:],\n",
    "                'time_offset': time_offset3,\n",
    "                'segmentID': 3\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No interval found for trip_id: {trip_id}\")\n",
    "\n",
    "        # Add segments to their respective lists\n",
    "        if len(first_segment['time_gap']) > 1:\n",
    "            road_list.append(first_segment)\n",
    "\n",
    "        if len(third_segment['time_gap']) > 1:\n",
    "            road_list.append(third_segment)\n",
    "\n",
    "        if len(second_segment['time_gap']) > 1:\n",
    "            second_segment_list.append(second_segment)\n",
    "\n",
    "    # Convert the lists to DataFrames\n",
    "    road_df = pd.DataFrame(road_list)\n",
    "    second_segment_df = pd.DataFrame(second_segment_list)\n",
    "\n",
    "    return road_df, second_segment_df\n",
    "\n",
    "road_df, second_segment_df = segment_trips(df, longest_intervals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_25.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_25.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_26.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_26.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_27.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_27.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_28.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_28.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_29.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_29.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_30.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_30.json'\n",
      "Segments 1 & 3 saved to 'Segmented Trial Week/Segmented_Trips_01_31.json'\n",
      "Segment 2 with wait times saved to 'Segmented Trial Week/Merchants_Segments_01_31.json'\n",
      "Index(['trip_id', 'time_gap', 'dist', 'trip_time', 'driverID', 'weekID',\n",
      "       'timeID', 'dateID', 'dist_gap', 'lats', 'lngs', 'time_offset',\n",
      "       'segmentID'],\n",
      "      dtype='object')\n",
      "Index(['trip_id', 'time_gap', 'dist', 'trip_time', 'driverID', 'weekID',\n",
      "       'timeID', 'dateID', 'dist_gap', 'lats', 'lngs', 'time_offset',\n",
      "       'merchant'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Remove trips with single 3rd segment occurences\n",
    "single_occurrence_trips = road_df['trip_id'].value_counts()\n",
    "single_occurrence_trips = single_occurrence_trips[single_occurrence_trips == 1].index\n",
    "\n",
    "filtered_single_trips_df = road_df[road_df['trip_id'].isin(single_occurrence_trips)]\n",
    "\n",
    "filtered_single_trips_segment3_df = filtered_single_trips_df[filtered_single_trips_df['segmentID'] == 1]\n",
    "\n",
    "# Remove these trips from road_df\n",
    "road_df = road_df[~road_df['trip_id'].isin(filtered_single_trips_segment3_df['trip_id'])]\n",
    "\n",
    "\n",
    "for date_id in range(24, 31):\n",
    "    # Filter road_df and second_segment_df for the current dateID\n",
    "    road_df_filtered = road_df[road_df['dateID'] == date_id]\n",
    "    second_segment_df_filtered = second_segment_df[second_segment_df['dateID'] == date_id]\n",
    "    \n",
    "    # Convert to JSON format\n",
    "    road_json = road_df_filtered.to_dict(orient='records')\n",
    "    second_seg_json = second_segment_df_filtered.to_dict(orient='records')\n",
    "\n",
    "    # Generate filenames with dateID + 1 \n",
    "    road_file_name = f'Segmented Trips/Segmented_Trips_01_{date_id + 1}.json'\n",
    "    segment_file_name = f'Segmented Trips/Merchants_Segments_01_{date_id + 1}.json'\n",
    "\n",
    "    # Save files\n",
    "    with open(road_file_name, 'w') as file:\n",
    "        for json_obj in road_json:\n",
    "            json.dump(json_obj, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "    with open(segment_file_name, 'w') as file:\n",
    "        for json_obj in second_seg_json:\n",
    "            json.dump(json_obj, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "    print(f\"Segments 1 & 3 saved to '{road_file_name}'\")\n",
    "    print(f\"Segment 2 with wait times saved to '{segment_file_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>weekID</th>\n",
       "      <th>timeID</th>\n",
       "      <th>dateID</th>\n",
       "      <th>time_offset</th>\n",
       "      <th>segmentID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "      <td>64611.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.245647</td>\n",
       "      <td>545.493941</td>\n",
       "      <td>2.995945</td>\n",
       "      <td>882.030065</td>\n",
       "      <td>26.887450</td>\n",
       "      <td>462.391729</td>\n",
       "      <td>2.081147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.907593</td>\n",
       "      <td>437.792605</td>\n",
       "      <td>1.946319</td>\n",
       "      <td>280.359040</td>\n",
       "      <td>2.044701</td>\n",
       "      <td>537.200286</td>\n",
       "      <td>0.996710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.883395</td>\n",
       "      <td>208.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>686.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.373664</td>\n",
       "      <td>432.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>904.200000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.662286</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1094.900000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>815.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2872.321048</td>\n",
       "      <td>4980.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1437.400000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>6442.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dist     trip_time        weekID        timeID        dateID  \\\n",
       "count  64611.000000  64611.000000  64611.000000  64611.000000  64611.000000   \n",
       "mean       4.245647    545.493941      2.995945    882.030065     26.887450   \n",
       "std       25.907593    437.792605      1.946319    280.359040      2.044701   \n",
       "min        0.000000      0.000000      0.000000      3.000000     24.000000   \n",
       "25%        0.883395    208.500000      1.000000    686.000000     25.000000   \n",
       "50%        2.373664    432.000000      3.000000    904.200000     27.000000   \n",
       "75%        5.662286    780.000000      5.000000   1094.900000     29.000000   \n",
       "max     2872.321048   4980.000000      6.000000   1437.400000     30.000000   \n",
       "\n",
       "        time_offset     segmentID  \n",
       "count  64611.000000  64611.000000  \n",
       "mean     462.391729      2.081147  \n",
       "std      537.200286      0.996710  \n",
       "min        0.000000      1.000000  \n",
       "25%        0.000000      1.000000  \n",
       "50%      344.000000      3.000000  \n",
       "75%      815.000000      3.000000  \n",
       "max     6442.000000      3.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "road_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>weekID</th>\n",
       "      <th>timeID</th>\n",
       "      <th>dateID</th>\n",
       "      <th>time_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35009.0</td>\n",
       "      <td>35009.000000</td>\n",
       "      <td>35009.000000</td>\n",
       "      <td>35009.000000</td>\n",
       "      <td>35009.000000</td>\n",
       "      <td>35009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>587.805764</td>\n",
       "      <td>2.981176</td>\n",
       "      <td>874.914708</td>\n",
       "      <td>26.907024</td>\n",
       "      <td>254.751550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>392.575488</td>\n",
       "      <td>1.944636</td>\n",
       "      <td>282.744044</td>\n",
       "      <td>2.050928</td>\n",
       "      <td>251.245861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>677.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>498.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>785.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1090.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>358.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5824.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1429.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3415.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dist     trip_time        weekID        timeID        dateID  \\\n",
       "count  35009.0  35009.000000  35009.000000  35009.000000  35009.000000   \n",
       "mean       0.0    587.805764      2.981176    874.914708     26.907024   \n",
       "std        0.0    392.575488      1.944636    282.744044      2.050928   \n",
       "min        0.0     10.000000      0.000000      3.000000     24.000000   \n",
       "25%        0.0    295.000000      1.000000    677.000000     25.000000   \n",
       "50%        0.0    498.000000      3.000000    898.000000     27.000000   \n",
       "75%        0.0    785.000000      5.000000   1090.000000     29.000000   \n",
       "max        0.0   5824.000000      6.000000   1429.000000     30.000000   \n",
       "\n",
       "        time_offset  \n",
       "count  35009.000000  \n",
       "mean     254.751550  \n",
       "std      251.245861  \n",
       "min        0.000000  \n",
       "25%       77.000000  \n",
       "50%      195.000000  \n",
       "75%      358.000000  \n",
       "max     3415.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_segment_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
