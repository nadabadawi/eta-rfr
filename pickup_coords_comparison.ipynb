{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory and file name pattern\n",
    "directory = 'Cleaning/trial_week/clean_data'\n",
    "file_base = 'clean_2024-01-'\n",
    "\n",
    "# Generate the list of file names\n",
    "files_names = [f'{file_base}{i}.json' for i in range(28, 29)]  # day 28 only\n",
    "\n",
    "# Generate the list of file directories\n",
    "files = [os.path.join(directory, file_name) for file_name in files_names]\n",
    "\n",
    "data = []\n",
    "\n",
    "# Load data from each JSON file\n",
    "for file_name in files:\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop the 'time_diff' attribute if it exists\n",
    "if 'time_diff' in df.columns:\n",
    "    df.drop('time_diff', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Calculate trip time in seconds\n",
    "df['trip_time'] = df['time']\n",
    "\n",
    "# Now, drop the 'time' column\n",
    "df.drop(columns=['time'], inplace=True)\n",
    "\n",
    "# Drop rows with missing values (if any)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Load pooling data\n",
    "pooling_file_path = 'Cleaning/anon_pooling_jan_24_amman.csv' \n",
    "df_pooling = pd.read_csv(pooling_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Stationary Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationary intervals saved to stationary_intervals_all_trips.csv\n"
     ]
    }
   ],
   "source": [
    "def detect_stationary_intervals(df, output_path='stationary_intervals_all_trips.csv'):\n",
    "    results = []\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        lats = np.array(row['lats'])\n",
    "        lngs = np.array(row['lngs'])\n",
    "        time_gap = np.array(row['time_gap'])\n",
    "        \n",
    "        # Compute differences between consecutive points directly from lat/lng arrays\n",
    "        lat_diff = np.abs(np.diff(lats))\n",
    "        lng_diff = np.abs(np.diff(lngs))\n",
    "\n",
    "        # Identify stationary intervals (where there is no change in both lat and lng)\n",
    "        stationary_indices = np.where((lat_diff == 0) & (lng_diff == 0))[0]\n",
    "        \n",
    "        # Group consecutive stationary indices into intervals\n",
    "        if len(stationary_indices) > 0:\n",
    "            start_idx = stationary_indices[0]\n",
    "            for i in range(1, len(stationary_indices)):\n",
    "                # If the current index is not consecutive, close the interval\n",
    "                if stationary_indices[i] != stationary_indices[i - 1] + 1:\n",
    "                    end_idx = stationary_indices[i - 1]\n",
    "                    # Save the interval only if it has more than one index\n",
    "                    if end_idx > start_idx:\n",
    "                        interval = {\n",
    "                            \"trip_id\": trip_id,\n",
    "                            \"start_idx\": int(start_idx),\n",
    "                            \"end_idx\": int(end_idx),\n",
    "                            \"start_lat\": float(lats[start_idx]),\n",
    "                            \"start_lng\": float(lngs[start_idx]),\n",
    "                            \"end_lat\": float(lats[end_idx]),\n",
    "                            \"end_lng\": float(lngs[end_idx]),\n",
    "                            \"time_elapsed\": float(time_gap[end_idx] - time_gap[start_idx])\n",
    "                        }\n",
    "                        results.append(interval)\n",
    "                    # Start a new interval\n",
    "                    start_idx = stationary_indices[i]\n",
    "            # Add the last interval if it has more than one index\n",
    "            end_idx = stationary_indices[-1]\n",
    "            if end_idx > start_idx:\n",
    "                interval = {\n",
    "                    \"trip_id\": trip_id,\n",
    "                    \"start_idx\": int(start_idx),\n",
    "                    \"end_idx\": int(end_idx),\n",
    "                    \"start_lat\": float(lats[start_idx]),\n",
    "                    \"start_lng\": float(lngs[start_idx]),\n",
    "                    \"end_lat\": float(lats[end_idx]),\n",
    "                    \"end_lng\": float(lngs[end_idx]),\n",
    "                    \"time_elapsed\": float(time_gap[end_idx] - time_gap[start_idx])\n",
    "                }\n",
    "                results.append(interval)\n",
    "\n",
    "    # Convert results to a DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Stationary intervals saved to {output_path}\")\n",
    "    return results_df\n",
    "\n",
    "stationary_df = detect_stationary_intervals(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_longest_stationary_interval(stationary_df):\n",
    "    # Find the longest stationary interval for each trip_id based on time_elapsed\n",
    "    longest_intervals = stationary_df.loc[stationary_df.groupby('trip_id')['time_elapsed'].idxmax()]\n",
    "\n",
    "    longest_intervals.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return longest_intervals\n",
    "\n",
    "longest_intervals_df = extract_longest_stationary_interval(stationary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Identify Non-matching Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to combined_trip_data.csv\n"
     ]
    }
   ],
   "source": [
    "def create_combined_csvs(longest_intervals_df, pooling_df, output_path1='combined_trip_data.csv'):\n",
    "    \n",
    "    # Rename 'booking_id' to 'trip_id' in pooling_df for consistency\n",
    "    pooling_df = pooling_df.rename(columns={'booking_id': 'trip_id'})\n",
    "    \n",
    "    # Create csv file to combine pooling_df and longest_intervals_df\n",
    "    combined_df = (\n",
    "        longest_intervals_df[['trip_id', 'start_lat', 'start_lng']]\n",
    "        .merge(pooling_df[['trip_id', 'pickup_latitude', 'pickup_longitude']], on='trip_id', how='inner')\n",
    "    )\n",
    "\n",
    "    # Rename columns for clarity in the combined data\n",
    "    combined_df = combined_df.rename(columns={\n",
    "        'start_lat': 'stationary_df_lat',\n",
    "        'start_lng': 'stationary_df_lng',\n",
    "        'pickup_latitude': 'pooling_df_lat',\n",
    "        'pickup_longitude': 'pooling_df_lng'\n",
    "    })\n",
    "\n",
    "    # Save the combined data to CSV\n",
    "    combined_df.to_csv(output_path1, index=False)\n",
    "    print(f\"Combined data saved to {output_path1}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "combined_df = create_combined_csvs(longest_intervals_df, df_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-matching coordinate entries saved to non_matching_coordinates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/pwyj2dkd2_j_jhxl5z4j87k80000gn/T/ipykernel_58795/4207462065.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_matching_df['distance_km'] = haversine(\n"
     ]
    }
   ],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "\n",
    "    # Compute differences\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Apply Haversine formula\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "def extract_non_matching_coordinates(combined_df):\n",
    "    # Identify rows where the stationary coordinates do not match pooling coordinates\n",
    "    non_matching_df = combined_df[\n",
    "        (combined_df['stationary_df_lat'] != combined_df['pooling_df_lat']) |\n",
    "        (combined_df['stationary_df_lng'] != combined_df['pooling_df_lng'])\n",
    "    ]\n",
    "\n",
    "    # calculate the distance between the coordinates\n",
    "    non_matching_df['distance_km'] = haversine(\n",
    "        non_matching_df['stationary_df_lat'],\n",
    "        non_matching_df['stationary_df_lng'],\n",
    "        non_matching_df['pooling_df_lat'],\n",
    "        non_matching_df['pooling_df_lng']\n",
    "    )\n",
    "\n",
    "    return non_matching_df\n",
    "\n",
    "\n",
    "# Save non-matching entries to CSV\n",
    "non_matching_entries = extract_non_matching_coordinates(combined_df)\n",
    "non_matching_entries.to_csv('non_matching_coordinates.csv', index=False)\n",
    "print(\"Non-matching coordinate entries saved to non_matching_coordinates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Trips into 3 Segments [Driver-2-Merchant (1), Wait-Time-At-Merchant (2), Merchant-2-Customer (3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments 1 & 3 saved to 'Segmented_Trips_Filtered_01_28.json'\n",
      "Segment 2 with wait times saved to 'Merchants_Segments_Filtered_01_28.json'\n"
     ]
    }
   ],
   "source": [
    "def segment_trips(df, longest_intervals_df):\n",
    "    road_list = []\n",
    "    second_segment_list = []\n",
    "\n",
    "    # Iterate over each entry in the df dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        trip_id = row['trip_id']\n",
    "        interval = longest_intervals_df[longest_intervals_df['trip_id'] == trip_id]\n",
    "        lats = row['lats']\n",
    "        lngs = row['lngs']\n",
    "        time_gap = row['time_gap']\n",
    "        dist_gap = row['dist_gap']\n",
    "\n",
    "        if not interval.empty:\n",
    "            start_idx = interval['start_idx'].values[0]\n",
    "            end_idx = interval['end_idx'].values[0]\n",
    "\n",
    "            # Segment the trip into three parts\n",
    "            first_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': time_gap[:start_idx],\n",
    "                'dist': dist_gap[start_idx - 1],\n",
    "                'trip_time': time_gap[start_idx - 1],\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID'],\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': dist_gap[:start_idx],\n",
    "                'lats': lats[:start_idx],\n",
    "                'lngs': lngs[:start_idx],\n",
    "                'segmentID': 1\n",
    "            }\n",
    "            \n",
    "            stationary_time_gap = time_gap[start_idx:end_idx+1]\n",
    "            # subtract each element in the new_time_gap list by the first element in the list\n",
    "            stationary_time_gap = [time - stationary_time_gap[0] for time in stationary_time_gap]\n",
    "\n",
    "            stationary_dist_gap = dist_gap[start_idx:end_idx+1]\n",
    "            # subtract each element in the new_dist_gap list by the first element in the list\n",
    "            stationary_dist_gap = [dist - stationary_dist_gap[0] for dist in stationary_dist_gap]\n",
    "        \n",
    "            second_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': stationary_time_gap,\n",
    "                'dist': stationary_dist_gap[-1],\n",
    "                'trip_time': stationary_time_gap[-1],\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID'],\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': stationary_dist_gap,\n",
    "                'lats': lats[start_idx:end_idx+1],\n",
    "                'lngs': lngs[start_idx:end_idx+1],\n",
    "            }\n",
    "            \n",
    "            new_time_gap = time_gap[end_idx+1:]\n",
    "            # subtract each element in the new_time_gap list by the first element in the list\n",
    "            new_time_gap = [time - new_time_gap[0] for time in new_time_gap]\n",
    "\n",
    "            new_dist_gap = dist_gap[end_idx+1:]\n",
    "            # subtract each element in the new_dist_gap list by the first element in the list\n",
    "            new_dist_gap = [dist - new_dist_gap[0] for dist in new_dist_gap]\n",
    "            \n",
    "\n",
    "            third_segment = {\n",
    "                'trip_id': trip_id,\n",
    "                'time_gap': new_time_gap,\n",
    "                'dist': new_dist_gap[-1],\n",
    "                'trip_time': new_time_gap[-1],\n",
    "                'driverID': row['driverID'],\n",
    "                'weekID': row['weekID'],\n",
    "                'timeID': row['timeID'],\n",
    "                'dateID': row['dateID'],\n",
    "                'dist_gap': new_dist_gap,\n",
    "                'lats': lats[end_idx+1:],\n",
    "                'lngs': lngs[end_idx+1:],\n",
    "                'segmentID': 3\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No interval found for trip_id: {trip_id}\")\n",
    "\n",
    "        # add the first and third segments to the road_list\n",
    "        # check if the time_gap of first_segment is empty or not\n",
    "        if len(first_segment['time_gap']) > 1:\n",
    "            road_list.append(first_segment)\n",
    "\n",
    "        if len(third_segment['time_gap']) > 1:\n",
    "            road_list.append(third_segment)\n",
    "\n",
    "        # add the second segment to the second_segment_list\n",
    "        if len(second_segment['time_gap']) > 1:\n",
    "            second_segment_list.append(second_segment)\n",
    "\n",
    "    # Convert the lists to DataFrames\n",
    "    road_df = pd.DataFrame(road_list)\n",
    "    second_segment_df = pd.DataFrame(second_segment_list)\n",
    "\n",
    "    return road_df, second_segment_df\n",
    "\n",
    "road_df, second_segment_df = segment_trips(df, longest_intervals_df)\n",
    "\n",
    "\n",
    "\n",
    "road_json = road_df.to_dict(orient='records')\n",
    "second_seg_json = second_segment_df.to_dict(orient='records')\n",
    "\n",
    "with open('Segmented_Trips_Filtered_01_28.json', 'w') as file:\n",
    "    for json_obj in road_json:\n",
    "        json.dump(json_obj, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "with open('Merchants_Segments_Filtered_01_28.json', 'w') as file:\n",
    "    for json_obj in second_seg_json:\n",
    "        json.dump(json_obj, file)\n",
    "        file.write('\\n')\n",
    "        \n",
    "print(\"Segments 1 & 3 saved to 'Segmented_Trips_Filtered_01_28.json'\")\n",
    "print(\"Segment 2 with wait times saved to 'Merchants_Segments_Filtered_01_28.json'\")\n",
    "\n",
    "\n",
    "# print(road_df.columns)\n",
    "# print(second_segment_df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
